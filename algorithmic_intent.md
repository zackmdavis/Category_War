## Algorithmic Intent: A Hansonian Generalized Anti-Zombie Principle

> "Why didn't you tell him the truth? Were you afraid?"
>
> "I'm not _afraid_. I _chose_ not to tell him, because I anticipated negative consequences if I did so."
>
> "What do you think 'fear' _is_, exactly?"

It's tempting to think that consciousness isn't part of the physical universe. Seemingly, we can imagine a world _physically_ identically to our own—the same atom-configurations evolving under the same laws of physics—but with no _consciousness_, a world inhabited by [philosophical "zombies"](https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies) who move and talk, but only as mere automatons, without the spark of _mind_ within.

It can't actually work that way. When we _talk_ about consciousness, we do so with our merely physical lips or merely physical keyboards. The causal explanation for talk about consciousness has to _either_ exist entirely within physics (in which case anything we say about consciousness is causally unrelated to consciousness, which is absurd), _or_ there needs to be some place where the laws of physics are violated as the immaterial soul is observed to be "tugging" on the brain (which is in-principle experimentally detectable). Zombies can't exist.

But if consciousness exists within physics, it should respect a certain ["locality"](https://www.lesswrong.com/posts/XDkeuJTFjM9Y2x6v6/which-basis-is-more-fundamental), a [Generalized Anti-Zombie Principle](https://www.lesswrong.com/posts/kYAuNJX2ecH2uFqZ9/the-generalized-anti-zombie-principle): if the configuration-of-matter that _is you_, is conscious, then _almost_-identical configurations should also be conscious for _almost_ the same reasons. An artificial neuron that implements the same input-output relationships as a biological one, would "play the same role" within the brain, which would continue to compute the same externally-observable behavior.

We don't want to push the Generalized Anti-Zombie Principle so far as to say that only externally-observable behavior matters and internal mechanisms don't matter at all, because substantively different internal mechanisms could compute the same behavior. Prosaically, [acting](https://en.wikipedia.org/wiki/Acting) exists: even the best method actors aren't really occupying the same mental state that the characters they portray would be in. In the limit, we could (pretend that we could) imagine [an incomprehensibly vast Giant Lookup Table](https://www.lesswrong.com/posts/k6EPphHiBH4WWYFCj/gazp-vs-glut) that has stored the outputs that a conscious mind would have produced in response to any input. Is such a Giant Lookup Table—an entirely static mapping of inputs to outputs—conscious? Really?

But this thought experiment requires us to posit the existence of a Giant Lookup Table that _just happens_ to mimic the behavior of a conscious mind. _Why_ would that happen? Why would that _actually_ happen, in the real world? (Or the closest possible world large enough to contain the Giant Lookup Table.) "Just assume it happened by coincidence, for the sake of the thought experiment" is unsatisfying, because that kind of arbitrary [thermodynamic](https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition) miracle doesn't help us understand what kind of cognitive work the ordinary [simple concept](https://www.lesswrong.com/posts/82eMd5KLiJ5Z6rTrr/superexponential-conceptspace-and-simple-words) of _consciousness_ is doing for us. You can _assume_ that a broken and scrambled egg will spontaneously reassemble itself for the sake of a thought experiment, but the interpretation of your thought-experimental results may seem tendentious given that we have [Godlike confidence](https://www.lesswrong.com/posts/q7Me34xvSG3Wm97As/but-there-s-still-a-chance-right) that [you will never, ever see that happen in the real world](https://www.lesswrong.com/posts/zFuCxbY9E2E8HTbfZ/perpetual-motion-beliefs).

The [_hard_ problem of consciousness](http://www.scholarpedia.org/article/Hard_problem_of_consciousness) is still confusing unto me—it [_seems_ impossible](https://www.lesswrong.com/posts/XzrqkhfwtiSDgKoAF/wrong-questions) that any arrangement of mere matter could add up to the ineffable _qualia_ of subjective experience. But the easier and yet clearly _somehow_ related problem of how mere matter can do information-processing—can do things like construct "models" by [using sensory data to correlate its internal state with the state of the world](https://www.lesswrong.com/posts/6s3xABaXKPdFwA3FS/what-is-evidence)—seems understandable, and a lot of our ordinary _use_ of the concept of _consciousness_ necessarily deals with the "easy" problems, like how perception works or how to [interpret people's self-reports](https://en.wikipedia.org/wiki/Heterophenomenology), even if we [can't _see_ the identity](https://www.lesswrong.com/posts/KmghfjH6RgXvoKruJ/hand-vs-fingers) between the hard problem and the sum of all the easy problems. Whatever the true referent of "consciousness" is—however confused our current concept of it may be—it's going to be, among other things, the cause of our [_thinking_ that we have](https://www.lesswrong.com/posts/rQEwySCcLtdKHkrHp/righting-a-wrong-question) "consciousness."

If I were to punch you in the face, I can [anticipate the experience](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences) of you reacting somehow—perhaps by saying, "Ow, that really hurt! I'm perceiving an ontologically-basic _quale_ of pain right now! I hereby commit to extract a costly revenge on you if you do that again, even at disproportionate cost to myself!" The fact that the human brain has the detailed functional structure to compute that _kind_ of response, whereas rocks and trees don't, is why we can be confident that [rocks and trees don't secretly have minds like ours](https://www.lesswrong.com/posts/f4RJtHBPvDRJcCTva/when-anthropomorphism-became-stupid).

So far, so standard. [(Read the Sequences!)](https://www.readthesequences.com/) My interest today is in exploring how well this style of argument applies to other concepts besides _consciousness_—seeking, perhaps, a Generalized Generalized Anti-Zombie Principle.

Consider the idea of _sorting_. Human alphabets aren't just a set of symbols—we also have a concept of the alphabet coming in some canonical _order_. The order of the alphabet doesn't play any role in the written language itself: you wouldn't have trouble reading books from an alternate world where the order of the Roman alphabet ran _KUWONSEZYFIJTABHQGPLCMVDXR_, but all English words were the same—but you would have trouble _finding_ the books on a shelf that wasn't sorted in the order you're used to. Sorting is useful because it lets us find things more easily: "The title I'm looking for starts with a _P_, but the book in front of me starts with a _B_; skip ahead" is faster than "look at every book until you find the one".

In the days before computers, the work of sorting was always done by humans: if you want your physical bookshelf to be alphabetized, you probably don't have a lot of other options than manually handling the books yourself ("This title starts with a _Pl_; I should put it ... da da da _here_, after this title starting with _Pe_ but before its neighbor starting with _Po_"). But the _computational work_ of sorting is simple enough that we can program computers to do it and _prove [theorems](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms))_ about what is being accomplished, without getting confused about the [sacred mystery](https://www.lesswrong.com/posts/6i3zToomS86oj9bS6/mysterious-answers-to-mysterious-questions) of sorting-ness.

Very different systems can perform the work of sorting, but whether it's a human tidying her bookshelf, or a [punchcard-sorting machine](https://en.wikipedia.org/wiki/IBM_card_sorter), or a modern computer sorting in RAM, it's useful to have a [short word](https://www.lesswrong.com/posts/soQX8yXLbKy7cFvy8/entropy-and-short-codes) to describe _processes_ that "take in" some list of elements, and "output" a list with the same elements ordered with respect to some criterion, for which we can know that the theorems we prove about sorting-in-general will [apply to any system](http://zackmdavis.net/blog/2012/07/an-idea-for-a-psychology-experiment/) that implements sorting. (For example, sorting processes that can [only compare two items to check which is "greater"](https://en.wikipedia.org/wiki/Comparison_sort) (as opposed to being able to [exploit more detailed prior information about the distribution of elements](https://en.wikipedia.org/wiki/Sorting_algorithm#Non-comparison_sorts)) can expect to have to perform $n \log n$ comparisons, where $n$ is the length of the list.)

Someone who wasn't familiar with computers might refuse to recognize sorting algorithms as _real_ sorting, as opposed to mere ["artificial sorting"](https://www.lesswrong.com/posts/YhgjmCxcQXixStWMC/artificial-addition). After all, a human sorting her bookshelf _intends_ to put the books in order, whereas the computer is just an automaton following instructions, and doesn't intend anything at all—a zombie sorter!

But this position is kind of silly, a [gerrymandered concept definition](https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries). To be sure, it's true that the internal workings of the human are _very_ different from that of the computer. The human wasn't special-purpose programmed to sort and is necessarily doing a lot _more_ things. The whole _modality_ of visual perception, whereby photons bouncing off a physical copy of _Rationality: AI to Zombies_ and absorbed by the human's retina are interpreted as evidence to construct a mental representation of the book in physical reality, whose "title" "begins" with an "R", is _much more complicated_ than just storing the bit-pattern 1010010 (the [ASCII](https://en.wikipedia.org/wiki/ASCII) code for _R_) in RAM. Nor does the computer have the subjective experience of eagerly looking forward to how much easier it will be to find things after the bookshelf is sorted. The human also probably won't perform the exact same sequence of comparisons as a computer program implementing [quicksort](https://en.wikipedia.org/wiki/Quicksort)—which _also_ won't perform the same sequence of comparisons as a _different_ program implementing [merge sort](https://en.wikipedia.org/wiki/Merge_sort). But the comparisons—the act of taking two _things_ and placing them somewhere that _depends_ on which one is "greater", need to happen _in order to get the right answer_.

The concept of "sorting into alphabetical order" may have been invented before our concept of "computers", but the [most natural concept](https://www.lesswrong.com/posts/d5NyJ2Lf6N22AD9PB/where-to-draw-the-boundary) of sorting includes computers performing quicksort, merge sort, _&c._., despite the lack of intent. We might say that intent is epiphenominal _with respect to_ sorting.

But even if we can understand _sorting_ without understanding intent, intent isn't epiphenominal _to the universe_. Intent is part of [the fabric of](https://www.lesswrong.com/posts/h6fzC6wFYFxxKDm8u/the-fabric-of-real-things) [stuff that makes stuff happen](https://www.lesswrong.com/posts/NhQju3htS9W6p6wE6/stuff-that-makes-stuff-happen): there are sensory experiences that will cause you to usefully attribute _intent_ to some physical systems and not others.

Specifically, whatever "intent" is—however confused our current concept of it may be—it's going to be, among other things, the cause of [optimized](https://www.lesswrong.com/posts/D7EcMhL26zFNbJ3ED/optimization) behavior. We can think of something as an optimization process if it's easier to predict its effects on the world by attributing _goals_ to it, rather than by simulating its detailed actions and internal state. ["To figure out a strange plot, look at what happens, then ask who benefits."](https://www.hpmor.com/chapter/97)

Alex Flint [identifies _robustness to perturbations_ as another feature of optimizing systems](https://www.lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1). If you scrambled the books on the shelf while the human was taking a bathroom break away from sorting, when she came back she would _notice_ the rearranged books, and sort them again—that's because she _intends_ to achieve the outcome of the shelf being sorted. Sorting algorithms don't, in general, have this property: if you shuffle a subarray in memory that the operation of the algorithm assumes has already been sorted, there's nothing in the code to notice or care that the "intended" output was not achieved.

Note that this is a "behaviorist", "third person" perspective: we're [not talking about some subjective feeling](https://www.lesswrong.com/posts/JDLKjYKDb5ohTAY45/bad-intent-is-a-disposition-not-a-feeling) of _intending_ something, just systems that systematically steer reality into otherwise-improbable states that rank high with respect to some preference ordering.

Robin Hanson often writes about [hidden motives in everyday life](http://elephantinthebrain.com/), advancing the thesis that [the criteria that control our decisions aren't the same as](https://www.lesswrong.com/posts/i6fKszWY6gLZSX2Ey/fake-optimization-criteria) the high-minded story we tell other people, and even the story we represent to ourselves. If you take a strictly first-person perspective on _intent_, the very idea of hidden motives seems absurd—a contradiction in terms. What would it even _mean_, to intend something without being aware of it? How would you _identify_ an alleged hidden motive?

The answer is that positing hidden motives can simplify our predictions of behavior. It can be easier to "look backwards" from what goals the behavior achieves, and _continues_ to achieve in the presence of perturbations, than to "look forwards" from a detailed model of the underlying psychological mechanisms (which are [typically unknown](https://www.lesswrong.com/posts/vNBxmcHpnozjrJnJP/no-one-knows-what-science-doesn-t-know)).

Hanson and coauthor Kevin Simler discuss the example of nonhuman primates grooming each other—manually combing each other's fur to remove dirt and parasites. One might assume that the function of grooming is just what it appears to be: hygiene. But that doesn't explain why primates spend more time grooming than they need to, why they predominately groom others rather than themselves, and why the amount of time a species spends grooming is unrelated to the amount of hair it has to groom, but _is_ related to the size of social groupings. These anomalies make more sense if we posit that grooming has been optimized for social-political functions, to provide a _credible_ signal of trust.[^elephant] (The [signal has to cost something](https://en.wikipedia.org/wiki/Signalling_theory)—in this case, time—in order for it to not be profitable to fake.) The hygienic function of grooming isn't unreal—parasites do in fact get removed—but the world [_looks more confusing_](https://www.lesswrong.com/posts/5JDkW4MYXit2CquLs/your-strength-as-a-rationalist) if you assume the behavior is optimized solely for hygiene.

[^elephant]: Robin Hanson and Kevin Simler, _The Elephant in the Brain: Hidden Motives in Everyday Life_, Ch. 1, "Animal Behavior"

This kind of multiplicity of purposes is ubiquitous: thus, [nobody does the thing they are supposedly doing](https://www.lesswrong.com/posts/8iAJ9QsST9X9nzfFy/nobody-does-the-thing-that-they-are-supposedly-doing): [politics isn't about policy](http://www.overcomingbias.com/2008/09/politics-isnt-a.html), [school is not about learning](http://www.overcomingbias.com/2010/08/school-isnt-about-learning.html), [medicine is not about health](http://www.overcomingbias.com/2008/03/showing-that-yo.html), _&c._

There are functional reasons for some of the purposes of social behavior to be covert, to conceal or misrepresent information that it wouldn't be profitable for others to know, and when lying is hard because it's too expensive to maintain two mental representations (the real map for ourselves, and a fake map for our victims). This is sometimes explained as, "We self-deceive in order to better deceive others," but I fear that this formulation might suggest more "central planning" than is really necessary: "self-deception" can _arise_ from different parts of the mind working at cross-purposes.

Ziz [discusses the example of a father](https://sinceriously.fyi/false-faces/) attempting to practice [nonviolent communication](https://en.wikipedia.org/wiki/Nonviolent_Communication) with his unruly teenage son: the father wants to have an honest and peaceful discussion of feelings and needs, but is afraid he'll lose control and become angry and threatening.

But angry threats aren't just a _random mistake_, in the way it's a random mistake if I forget to carry the one while adding 143 + 28. Random mistakes don't serve a purpose and don't resist correction: there's no plausible reason for me to _want_ the incorrect answer 143 + 28 = 161, and if you say, "Hey, you forgot to carry the one," I'll almost certainly just say "Oops" and get it right the second time. Even if I'm more likely to make arithmetic errors when I'm tired, the errors probably won't correlate in a way that _steers the future_ in a particular direction: you can't use information about _what I want_ to make better predictions about what _specific_ errors I'll make, nor use observations of specific errors to infer what I want.

In contrast, the father is likely to "lose control" and make angry threats precisely _when_ peaceful behavior _isn't getting him what he wants_. That's what anger is _designed to do_ from an [evolutionary perspective](https://www.lesswrong.com/posts/epZLSoNvjW53tqNj9/evolutionary-psychology): [threaten to impose costs or withhold benefits to induce conspecifics to place more weight on the angry individual's welfare](https://www.cep.ucsb.edu/topics/anger.htm).

Another example: _Less Wrong_ commenter Caravelle [tells a story about finding a loophole in an online game](https://www.lesswrong.com/posts/DSnamjnW7Ad8vEEKd/trivers-on-self-deception?commentId=CandwLBdJXXq7Qxet), and being _outraged_ to later be accused of cheating by the game administrators—only in retrospect remembering that, on first discovering the hack, they had specifically _told_ their teammates not to tell the administrators. The earlier Caravelle-who-discovered-the-bug must have known that the admins wouldn't allow it (or else why instruct teammates to keep quiet about it?), but the later Caravelle-who-exploited-the-bug was able to protest with perfect sincerity that they couldn't have known.

Another example: someone asks me an innocuous-as-far-as-they-know question that I don't feel like answering. Maybe we're making a cake, and I feel self-conscious about my lack of baking experience. You ask, "Why did you just add an eighth-cup of vanilla?" I initially mishear you as having said, "Did you just add ..." and reply, "Yes." It's only a moment later that I realize that _that's not what you asked_: you said "_Why_ did you ...", not "_Did_ you ...". But I don't correct myself, and you don't press the point. I am not a cognitive scientist and I don't _know_ what was really going on in my brain when I misheard you: maybe my audio processing is just slow. But it seems awfully _convenient_ for me that I momentarily misheard your question _specifically_ when I didn't want to answer it and thereby reveal that I don't know what I'm doing—almost as if some elephant in my brain bet that it could get away with pretending to mishear you, and the bet paid off.

The Hansonian Generalized Anti-Zombie Principle calls for us to posit _something like_ "intent" causally upstream of optimized behavior (even if the causal link might be complicated and we might be wrong about the details of what _intent_ is). You can't have a zombie that _just happens_ to systematically select actions that result in outcomes that rank high with respect to a compactly-describable preference ordering _for no reason_.

This is similar to how the ordinary Generalized Anti-Zombie Principle calls for us to posit some kind of "consciousness" casually upstream of reports of phenomenological experience (even if the causal link might be complicated and we might be wrong about the details of what _consciousness_ is). If you're _already_ familiar with conscious humans, then maybe you can specifically engineer a non-conscious chatbot that imitates the surface behaviors of humans talking about their phenomenological experiences, but you can't have a zombie that spontaneously talks about being conscious _for no reason_.

However, our existing language may lack the vocabulary to adequately describe optimized behavior that comes from a mixture of overt and hidden motives. Does the father _intend_ to blame and threaten? Did the gamer _intend_ to cheat? Was I only _pretending_ to mishear your question, rather than actually mishearing it? We want to say _No_—not in the same sense that someone consciously intends to sort her bookshelf. And yet it seems useful to have [short codewords](https://www.lesswrong.com/posts/soQX8yXLbKy7cFvy8/entropy-and-short-codes) to talk about the aspects of these behaviors that seem _optimized_: it's _not a coincidence_ when someone "loses control" and makes angry threats when being nice isn't getting them what they want, and it's _not a coincidence_ if they calm down if the threat succeeds.

As Jessica Taylor explains, when our existing language lacks the vocabulary to accommodate our expanded ontology in the wake of a new discovery, one strategy for adapting our language is to define new senses of existing words that [metaphorically extend the original meaning](https://www.lesswrong.com/posts/wR4PaDp2Knu5coeXx/metaphorical-extensions-and-conceptual-figure-ground). The statement "Ice is a form of water" might be new information to a child or a primitive AI who has already seen (liquid) water, and already seen ice, but didn't _know_ that the former turns into the latter when sufficiently cold.

The word _water_ in the sentence "Ice is a form of water" actually has a _different_ [extensional meaning](https://www.lesswrong.com/posts/HsznWM9A7NiuGsp28/extensions-and-intensions) than the word _water_ in the sentence "Water is a liquid", but both definitions can coexist—as long as we're careful to precisely [disambiguate which sense](https://www.lesswrong.com/posts/y5MxoeacRKKM3KQth/fallacies-of-compression) of the word is meant in contexts where [equivocation](https://www.lesswrong.com/posts/shoMpaoZypfkXv84Y/variable-question-fallacies) [could be deceptive](http://web.archive.org/web/20200529221511/https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/).

We might wish to apply a similar linguistic tactic in order to be able to concisely _talk_ about cases where we think someone's behavior is _optimized to achieve goals_, but the computation that determines the behavior isn't necessarily overt or conscious.

_Algorithmic_ seems like a promising candidate for a disambiguating adjective to make it clear that we're talking about _the optimization criteria implied by_ a system's inputs and outputs, rather than [what it subjectively feels like to be that system](https://www.lesswrong.com/posts/yA4gF5KrboK2m2Xu7/how-an-algorithm-feels-from-inside). We could then speak of an "algorithmic intent" that doesn't necessarily imply "(conscious) intent", similarly to how ice is a form of "water" despite not being "(liquid) water". We might similarly want to speak of algorithmic "honesty" (referring to [signals](https://www.lesswrong.com/posts/4hLcbXaqudM9wSeor/philosophy-in-the-darkest-timeline-basics-of-the-evolution) selected on the criterion of making receivers have more accurate beliefs), ["deception"](https://www.lesswrong.com/posts/fmA2GJwZzYtkrAKYJ/algorithms-of-deception) (referring to signals selected for producing _less_ accurate beliefs), or even "fraud" (_deception_ that moves resources to the agent sending the deceptive signal).

Some authors might admit the pragmatic usefulness of the metaphorical extension, but insist that the new usage be marked as "just a metaphor" with a prefix such as pseudo- or [_quasi-_](https://www.lesswrong.com/posts/FT9Lkoyd5DcCoPMYQ/partial-summary-of-debate-with-benquo-and-jessicata-pt-1?commentId=coWFfoYqdeuSPpTqe#vPekZcouSruiCco3c). But I claim that broad "algorithmic" senses of "mental" words like _intent_ often are more relevant and useful for making sense of the world than the original, narrower definitions that were invented by humans in the context of dealing with other humans, because the universe _in fact_ does not revolve around humans.

When a predatory [_Photuris_](https://en.wikipedia.org/wiki/Photuris) firefly [sends the mating signal of a different species](https://en.wikipedia.org/wiki/Aggressive_mimicry) of firefly in order to lure prey, I think it makes sense to straight-up call this [_deceptive_](https://en.wikipedia.org/wiki/Deception_in_animals) (rather than merely pseudo- or quasi-deceptive), even though fireflies don't have language with which to think the verbal thought, "And now I'm going to send another species's mating signal in order to lure prey ..."

When a [generative adversarial network](https://en.wikipedia.org/wiki/Generative_adversarial_network) learns to produce images of [realistic human faces](https://en.wikipedia.org/wiki/StyleGAN) [or anime characters](https://www.lesswrong.com/posts/mAduS8gYGFMZbNq5E/this-waifu-does-not-exist-100-000-stylegan-and-gpt-2-samples), it would in no way aid our understanding to insist that the system isn't _really_ "learning" just because it's not a human learning the way a human would—any more than it would to insist that quicksort isn't _really_ sorting. "Using exposure to data as an input into gaining capabilities" is a perfectly adequate definition of _learning_ in this context.

In a nearby possible future, when you sue a company for fraud because their advertising claimed that their product would disinfect wolf bites, but instead gave you cancer, we would hope that the court will not be persuaded if the company's defense-lawyer AI says, "But that advertisement was composed by filtering [GPT](https://www.lesswrong.com/tag/gpt)-5 output [for the version that increased sales the most](https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on)—at no point did any human form the _conscious intent_ to deceive you!"

Another possible concern with this proposed language usage is that if it's socially permissible to [attribute unconscious motives to interlocutors](https://web.archive.org/web/20200619200544/https://slatestarcodex.com/2019/07/17/caution-on-bias-arguments/), [people will abuse this](https://web.archive.org/web/20200619222332/https://slatestarcodex.com/2019/07/16/against-lie-inflation/) to selectively accuse their rivals of bad intent, leading to toxic social outcomes: there's no way for negatively-valenced intent-language like "fraud" or "deceive" to [stably have _denotative_ meanings](https://www.lesswrong.com/posts/N9oKuQKuf7yvCCtfq/can-crimes-be-discussed-literally) independently of questions of [who should be punished](https://www.lesswrong.com/posts/r2dTchodfqX4o5DYH/blame-games).

As far as I can tell, this concern is _correct_: in a human community of any appreciable size, if you let people [question the stories we tell about ourselves](http://unremediatedgender.space/2016/Sep/psychology-is-about-invalidating-peoples-identities/), you _are_ going to get acrimonious and not-readily-falsifiable accusations of bad intent. ("_Liar!_" "Huh? You can argue that I'm wrong, but I actually believe what I'm saying!" "Oh, maybe _consciously_, but I was accusing you of being an _algorithmic_ liar.")

Unfortunately, as an aspiring epistemic rationalist, [I'm _not allowed to care_](https://www.lesswrong.com/posts/bSmgPNS6MTJsunTzS/maybe-lying-doesn-t-exist#Appeals_to_Consequences_Are_Invalid) whether some descriptions might be socially harmful for a human community to adopt; I'm _only_ allowed to care about what descriptions [shorten the length of the message](https://www.lesswrong.com/posts/f4txACqDWithRi7hs/occam-s-razor) needed to describe my observations.

