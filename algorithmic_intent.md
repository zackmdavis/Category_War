## Algorithmic Intent: A Hansonian Generalized Anti-Zombie Principle

> "Why didn't you tell him the truth? Were you afraid?"

> "I'm not _afraid_. I _chose_ not to tell him, because I anticipated negative consequences if I did so."

> "What do you think 'fear' _is_, exactly?"

It's tempting to think that consciousness isn't part of the physical universe. Seemingly, we can imagine a world _physically_ identically to our own—the same atom-configurations evolving under the same laws of physics—but with no _consciousness_, a world inhabited by [philosophical "zombies"](https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies) who move and talk, but only as mere automatons, without the spark of _mind_ within.

It can't actually work that way. When we _talk_ about consciousness, we do so with our merely physical lips or merely physical keyboards. The causal explanation for talk about consciousness has to _either_ exist entirely within physics (in which case anything we say about consciousness is causally unrelated to consciousness, which is absurd), _or_ there needs to be some place where the laws of physics are violated as the immaterial soul is observed to be "tugging" on the brain (which is in-principle experimentally detectible). Zombies can't exist.

But if consciousness exists within physics, it should respect a certain ["locality"](https://www.lesswrong.com/posts/XDkeuJTFjM9Y2x6v6/which-basis-is-more-fundamental), a [Generalized Anti-Zombie Principle](https://www.lesswrong.com/posts/kYAuNJX2ecH2uFqZ9/the-generalized-anti-zombie-principle): if the configuration-of-matter that _is you_, is conscious, then _almost_-identical configurations should also be conscious for _almost_ the same reasons. An artificial neuron that implements the same input-output relationships as a biological one, would "play the same role" within the brain, which would continue to compute the same externally-observable behavior.

We probably don't want to push the Generalized Anti-Zombie Principle so far as to say that _only_ externally-observable behavior matters and internal mechanisms don't matter at all, because substantively different internal mechanisms could compute the same behavior. Prosaically, [acting](https://en.wikipedia.org/wiki/Acting) exists: even the best method actors aren't really occupying the same mental state as the characters they portray would be in. In the limit, we could (pretend that we could) imagine [an incomprehensibly vast Giant Lookup Table](https://www.lesswrong.com/posts/k6EPphHiBH4WWYFCj/gazp-vs-glut) that has stored the outputs that a conscious mind would have produced in response to any input. Is such a Giant Lookup Table—an entirely static mapping of inputs to outputs—conscious? Really?

But this thought experiment requires us to posit the existence of a Giant Lookup Table that _just happens_ to mimic the behavior of a conscious mind. _Why_ would that happpen? Why would that _actually_ happen, in the real world? (Or the closest possibly-impossible possible world large enough to contain the Giant Lookup Table.) "Just assume it happened by coincidence, for the sake of the thought experiment" is unsatisfying, because that kind of arbitrary [thermodynamic](https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition) miracle doesn't help us understand what the ordinary

[simple concept](https://www.lesswrong.com/posts/82eMd5KLiJ5Z6rTrr/superexponential-conceptspace-and-simple-words) of _consciousness_



You can _assume_ that a broken and scrambled egg will spontaneously reassemble itself for the sake of a thought experiment, 

https://www.lesswrong.com/posts/zFuCxbY9E2E8HTbfZ/perpetual-motion-beliefs [egg scrambling]

https://www.lesswrong.com/posts/f4RJtHBPvDRJcCTva/when-anthropomorphism-became-stupid

So far, so standard. [(Read the Sequences!)](https://www.readthesequences.com/)

My interest today is in exploring how well this style of argument applies 

—a Generalized Generalized Anti-Zombie Principle.

[sorting algorithm]

[expand the generalized principle beyond "consciousness", to "intent": "consciousness" makes preidctions about heterophenomenological behavior, "intent" also makes predictions about behavior]


> "Accuse me of _fraud_? How _dare_ you?! Sure, I'm not a perfect person free from all bias, but—"

> "Bias. Is that your word for having a disposition to communicate in a way that causes others to make incorrect predictions about the value you have to offer, in a direction that moves resources towards you?"

> "Uh. I guess you could say that."

> "What do you think 'fraud' _is_, exactly?"

[follow the improbability]


https://www.lesswrong.com/posts/N9oKuQKuf7yvCCtfq/can-crimes-be-discussed-literally


"The bureaucrat, police officer, teacher, judge, or cable television company representative functions as [...], not as a co-modeling and fully interacting person. His behaviors are governed by top-down rules and scripts, with human discretion eliminated as much as possible."

(Sarah Perry, "The Essence of Peopling")
