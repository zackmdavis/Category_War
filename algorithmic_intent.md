## Algorithmic Intent: A Hansonian Generalized Anti-Zombie Principle

> "Why didn't you tell him the truth? Were you afraid?"

> "I'm not _afraid_. I _chose_ not to tell him, because I anticipated negative consequences if I did so."

> "What do you think 'fear' _is_, exactly?"

It's tempting to think that consciousness isn't part of the physical universe. Seemingly, we can imagine a world _physically_ identically to our own—the same atom-configurations evolving under the same laws of physics—but with no _consciousness_, a world inhabited by [philosophical "zombies"](https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies) who move and talk, but only as mere automatons, without the spark of _mind_ within.

It can't actually work that way. When we _talk_ about consciousness, we do so with our merely physical lips or merely physical keyboards. The causal explanation for talk about consciousness has to _either_ exist entirely within physics (in which case anything we say about consciousness is causally unrelated to consciousness, which is absurd), _or_ there needs to be some place where the laws of physics are violated as the immaterial soul is observed to be "tugging" on the brain (which is in-principle experimentally detectible). Zombies can't exist.

But if consciousness exists within physics, it should respect a certain ["locality"](https://www.lesswrong.com/posts/XDkeuJTFjM9Y2x6v6/which-basis-is-more-fundamental), a [Generalized Anti-Zombie Principle](https://www.lesswrong.com/posts/kYAuNJX2ecH2uFqZ9/the-generalized-anti-zombie-principle): if the configuration-of-matter that _is you_, is conscious, then _almost_-identical configurations should also be conscious for _almost_ the same reasons. An artificial neuron that implements the same input-output relationships as a biological one, would "play the same role" within the brain, which would continue to compute the same externally-observable behavior.

We don't want to push the Generalized Anti-Zombie Principle so far as to say that only externally-observable behavior matters and internal mechanisms don't matter at all, because substantively different internal mechanisms could compute the same behavior. Prosaically, [acting](https://en.wikipedia.org/wiki/Acting) exists: even the best method actors aren't really occupying the same mental state that the characters they portray would be in. In the limit, we could (pretend that we could) imagine [an incomprehensibly vast Giant Lookup Table](https://www.lesswrong.com/posts/k6EPphHiBH4WWYFCj/gazp-vs-glut) that has stored the outputs that a conscious mind would have produced in response to any input. Is such a Giant Lookup Table—an entirely static mapping of inputs to outputs—conscious? Really?

But this thought experiment requires us to posit the existence of a Giant Lookup Table that _just happens_ to mimic the behavior of a conscious mind. _Why_ would that happpen? Why would that _actually_ happen, in the real world? (Or the closest possible world large enough to contain the Giant Lookup Table.) "Just assume it happened by coincidence, for the sake of the thought experiment" is unsatisfying, because that kind of arbitrary [thermodynamic](https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition) miracle doesn't help us understand what kind of cognitive work the ordinary [simple concept](https://www.lesswrong.com/posts/82eMd5KLiJ5Z6rTrr/superexponential-conceptspace-and-simple-words) of _consciousness_ is doing for us. You can _assume_ that a broken and scrambled egg will spontaneously reassemble itself for the sake of a thought experiment, but the interpretation of your thought-experimental results may seem tendentious given that we have [Godlike confidence](https://www.lesswrong.com/posts/q7Me34xvSG3Wm97As/but-there-s-still-a-chance-right) that [you will never, ever see that happen in the real world](https://www.lesswrong.com/posts/zFuCxbY9E2E8HTbfZ/perpetual-motion-beliefs).

The [_hard_ problem of consciousness](http://www.scholarpedia.org/article/Hard_problem_of_consciousness) is still confusing unto me—it [_seems_ impossible](https://www.lesswrong.com/posts/XzrqkhfwtiSDgKoAF/wrong-questions) that any arrangement of mere matter could add up to the ineffable _qualia_ of subjective experience. But the easier and yet clearly _somehow_ related problem of how mere matter can do information-processing—can do things like construct "models" by [using sensory data to correlate its internal state with the state of the world](https://www.lesswrong.com/posts/6s3xABaXKPdFwA3FS/what-is-evidence)—seems understanable, and a lot of our ordinary _use_ of the concept of _consciousness_ necessarily deals with the "easy" problems, like how perception works or how to [interpret people's self-reports](https://en.wikipedia.org/wiki/Heterophenomenology), even if we [can't _see_ the identity](https://www.lesswrong.com/posts/KmghfjH6RgXvoKruJ/hand-vs-fingers) between the hard problem and the sum of all the easy problems. Whatever the true referent of "consciousness" is—however confused our current concept of it may be—it's going to be, among other things, the cause of our [_thinking_ that we have](https://www.lesswrong.com/posts/rQEwySCcLtdKHkrHp/righting-a-wrong-question) "consciousness."

If I were to punch you in the face, I can [anticipate the experience](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences) of you reacting somehow—perhaps by saying, "Ow, that really hurt! I'm perceiving an ontologically-basic _quale_ of pain right now! I hereby commit to extract a costly revenge on you if you do that again, even at disproportionate cost to myself!" The fact that the human brain has the detailed functional structure to compute that _kind_ of response, whereas rocks and trees don't, is why we can be confident that [rocks and trees don't secretly have minds like ours](https://www.lesswrong.com/posts/f4RJtHBPvDRJcCTva/when-anthropomorphism-became-stupid).

So far, so standard. [(Read the Sequences!)](https://www.readthesequences.com/) My interest today is in exploring how well this style of argument applies to other concepts besides _consciousness_—seeking, perhaps, a Generalized Generalized Anti-Zombie Principle.

Consider the idea of _sorting_. Human alphabets aren't just a set of symbols—we also have a concept of the alphabet coming in some canonical _order_. The order of the alphabet doesn't play any role in the written language itself: you wouldn't have trouble reading books from an alternate world where the order of the Roman alphabet ran _KUWONSEZYFIJTABHQGPLCMVDXR_, but all English words were the same—but you would have trouble _finding_ the books on a shelf that wasn't sorted in the order you're used to. Sorting is useful because it lets us find things more easily: "The title I'm looking for starts with a _P_, but the book in front of me starts with a _B_; skip ahead" is faster than "look at every book until you find the one".

In the days before computers, the work of sorting was always done by humans: if you want your physical bookshelf to be alphabetized, you probably don't have a lot of other options than manually handling the books yourself ("This title starts with a _Pl_; I should put it ... da da da _here_, after this title starting with _Pe_ but before its neighbor starting with _Po_"). But the _computational work_ of sorting is simple enough that we can program computers to do it and _prove [theorems](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms))_ about what is being accomplished, without getting confused about the [sacred mystery](https://www.lesswrong.com/posts/6i3zToomS86oj9bS6/mysterious-answers-to-mysterious-questions) of sorting-ness.

Very different systems can perform the work of sorting, but whether it's a human tidying her bookshelf, or a [punchcard-sorting machine](https://en.wikipedia.org/wiki/IBM_card_sorter), or a modern computer sorting in RAM, it's useful to have a [short word](https://www.lesswrong.com/posts/soQX8yXLbKy7cFvy8/entropy-and-short-codes) to describe _processes_ that "take in" some list of elements, and "output" a list with the same elements ordered with respect to some criterion, for which we can know that the theorems we prove about sorting-in-general will [apply to any system](http://zackmdavis.net/blog/2012/07/an-idea-for-a-psychology-experiment/) that implements sorting. (For example, sorting processes that can [only compare two items to check which is "greater"](https://en.wikipedia.org/wiki/Comparison_sort) (as opposed to being able to [exploit more detailed prior information about the distribution of elements](https://en.wikipedia.org/wiki/Sorting_algorithm#Non-comparison_sorts)) can expect to have to perform $n \log n$ comparisons, where $n$ is the length of the list.)

Someone who wasn't familiar with computers might refuse to recognize sorting algorithms as _real_ sorting, as opposed to mere ["artificial sorting"](https://www.lesswrong.com/posts/YhgjmCxcQXixStWMC/artificial-addition). After all, a human sorting her bookshelf _intends_ to put the books in order, whereas the computer is just an automaton following instructions, and doesn't intend anything at all—a zombie sorter!

But this position is kind of silly, a [gerrymandered concept definition](https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries). To be sure, it's true that the internal workings of the human are _very_ different from that of the computer. The human wasn't special-purpose programmed to sort and is necessarily doing a lot _more_ things. The whole _modality_ of visual perception, whereby photons bouncing off a physical copy of _Rationality: AI to Zombies_ and absorbed by the human's retina are interpreted as evidence to construct a mental representation of the book in physical reality, whose "title" "begins" with an "R", is _much more complicated_ than just storing the bit-pattern 1010010 (the [ASCII](https://en.wikipedia.org/wiki/ASCII) code for _R_) in RAM. Nor does the computer have the subjective experience of eagerly looking forward to how much easier it will be to find things after the bookshelf is sorted. The human also probably won't perform the exact same sequence of comparisons as a computer program implementing [quicksort](https://en.wikipedia.org/wiki/Quicksort)—which _also_ won't perform the same sequence of comparisons as a _different_ program implementing [merge sort](https://en.wikipedia.org/wiki/Merge_sort). But the comparisons—the act of taking two _things_ and placing them somewhere that _depends_ on which one is "greater", need to happen _in order to get the right answer_.

The concept of "sorting into alphabetical order" may have been invented before our concept of "computers", but the [most natural concept](https://www.lesswrong.com/posts/d5NyJ2Lf6N22AD9PB/where-to-draw-the-boundary) of sorting includes computers performing quicksort, merge sort, _&c._., despite the lack of intent. We might say that intent is epiphenominal _with respect to_ sorting.

But even if we can understand _sorting_ without understanding intent, intent isn't epiphenominal _to the universe_. Intent is part of [the fabric of](https://www.lesswrong.com/posts/h6fzC6wFYFxxKDm8u/the-fabric-of-real-things) [stuff that makes stuff happen](https://www.lesswrong.com/posts/NhQju3htS9W6p6wE6/stuff-that-makes-stuff-happen): there are sensory experiences that will cause you to usefully attribute _intent_ to some physical systems and not others.

Specifically, whatever "intent" is—however confused our current concept of it may be—it's going to be, among other things, the cause of [optimized](https://www.lesswrong.com/posts/D7EcMhL26zFNbJ3ED/optimization) behavior. We can think of something as an optimization process if it's easier to predict its effects on the world by attributing _goals_ to it, rather than by simulating its detailed actions and internal state. ["To figure out a strange plot, look at what happens, then ask who benefits."](https://www.hpmor.com/chapter/97)

[Alex Flint identifies _robustness to perturbations_ as another feature of optimizing systems](https://www.lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1). If you scrambled the books on the shelf while the human was taking a bathroom break away from sorting, when she came back she would _notice_ the rearranged books, and sort them again—that's because she _intends_ to achieve the outcome of the shelf being sorted. Sorting algorithms don't, in general, have this property: if you shuffle a subarray in memory that the operation of the algorithm assumes has already been sorted, there's nothing in the code to notice or care that the "intended" output was not achieved.

Note that this is a "behaviorist", "third person" perspective: we're [not talking about some subjective feeling](http://benjaminrosshoffman.com/bad-faith-behavior-not-feeling/) of _intending_ something, just systems that systematically steer reality into otherwise-improbable states that rank high with respect to some preference ordering.

Robin Hanson often writes about [hidden motives in everyday life](http://elephantinthebrain.com/), advancing the thesis that people's actual goals are more selfish than the high-minded story we tell other people, and even the story we represent to ourselves. If you take a strictly first-person perspective on _intent_, the very idea of hidden motives seems absurd—a contradiction in terms. What would it even _mean_, to intend something without being aware of it? How would you _identify_ an alleged hidden motive?

The answer is that positing hidden motives can simplify our predictions of behavior. It can be easier to "look backwards" from what goals the behavior achieves, and _continues_ to achieve in the presence of perturbations, than to "look forwards" from a detailed model of the underlying psychological mechanisms (which are [typically unknown](https://www.lesswrong.com/posts/vNBxmcHpnozjrJnJP/no-one-knows-what-science-doesn-t-know)).

Hanson and coauthor Kevin Simler discuss the example of nonhuman primates grooming each other—manually combing each other's fur to remove dirt and parasites. One might assume that the function of grooming is just what it appears to be: hygiene. But that doesn't explain why primates spend more time grooming than they need to, why they predominately groom others rather than themselves, and why the amount of time a species spends grooming is unrelated to the amount of hair it has to groom, but _is_ related to the size of social groupings. These anomalies make more sense if we posit that grooming has been optimized for social functions, to provide a _credible_ signal of trust.[^elephant] (The [signal has to cost something](https://en.wikipedia.org/wiki/Signalling_theory)—in this case, time—in order for it to not be profitable to fake.) The hygienic function of grooming isn't unreal—parasites do in fact get removed—but the world [_looks more confusing_](https://www.lesswrong.com/posts/5JDkW4MYXit2CquLs/your-strength-as-a-rationalist) if you assume the behavior is optimized solely for hygiene.

[^elephant]: Robin Hanson and Kevin Simler, _The Elephant in the Brain: Hidden Motives in Everyday Life_, Ch. 1, "Animal Behavior"

This kind of multiplicity of purposes is ubiquitous: thus, [nobody does the thing they are supposedly doing](https://www.lesswrong.com/posts/8iAJ9QsST9X9nzfFy/nobody-does-the-thing-that-they-are-supposedly-doing): [politics isn't about policy](http://www.overcomingbias.com/2008/09/politics-isnt-a.html), [school is not about learning](http://www.overcomingbias.com/2010/08/school-isnt-about-learning.html), [medicine is not about health](http://www.overcomingbias.com/2008/03/showing-that-yo.html), _&c._

There are functional reasons for some of the purposes of social behavior to be covert, to conceal or misrepresent information that it wouldn't be profitable for others to know, and when lying is hard because it's too expensive to maintain two mental representations (the real map for ourselves, and a fake map for our victims). This is sometimes explained as, "We self-decieve in order to better decieve others," but I fear that this formulation might suggest more "central planning" than is actually necessary: "self-deception" can _arise_ from different parts of the mind working at cross-purposes.

[Ziz discusses the example of a father](https://sinceriously.fyi/false-faces/) attempting to practice [nonviolent communication](https://en.wikipedia.org/wiki/Nonviolent_Communication) with his unruly teenage son: the father wants to have an honest and peaceful discussion of feelings and needs, but is afraid he'll lose control and become angry and threatening.

But angry threats aren't just a _random mistake_, in the sense that it's a random mistake if I forget to carry the one while adding 143 + 28. Random mistakes don't serve a purpose and don't resist correction: there's no plausible reason for me to _want_ the incorrect answer 143 + 28 = 161, and if you say, "Hey, you forgot to carry the one," I'll almost certainly just say "Oops" and get it right the second time.

In contrast, the father is likely to "lose control" and make angry threats precisely _when_ peaceful behavior _isn't getting him what he wants_. That's what anger is _designed to do_ from an [evolutionary perspective](https://www.lesswrong.com/posts/epZLSoNvjW53tqNj9/evolutionary-psychology): [threaten to impose costs or withold benefits to induce conspecifics to place more weight on the angry individual's welfare](https://www.cep.ucsb.edu/topics/anger.htm).

[_Less Wrong_ commenter Caravelle tells a story about finding a loophole in an online game](https://www.lesswrong.com/posts/DSnamjnW7Ad8vEEKd/trivers-on-self-deception?commentId=CandwLBdJXXq7Qxet), and being _outraged_ to later be accused of cheating by the game administrators—only in retrospect remembering that, on first discovering the hack, they had specifically _said_ not to tell the administrators. The earlier Caravelle-who-discovered-the-bug must have known that the admins wouldn't allow it (or else why instruct teammates to keep quiet about it?), but the later Caravelle-who-exploited-the-bug was able to protest with perfect sincerity that they couldn't have known.

Our ordinary language may lack the vocabulary to adequately describe cases such as these where people act from a mixture of overt and hidden motives. Does the father _intend_ to blame and threaten? Did the gamer _intend_ to cheat? We want to say _No_—not in the same sense that someone consciously intends to sort her bookshelf. And yet in order to [compress the length of the message needed to describe our observations](https://www.lesswrong.com/posts/f4txACqDWithRi7hs/occam-s-razor), it seems useful to have [short codewords](https://www.lesswrong.com/posts/soQX8yXLbKy7cFvy8/entropy-and-short-codes) to talk about the aspects of these behaviors that seem _optimized_: it's _not a coincidence_ when someone "loses control" and makes angry threats when being nice isn't getting them what they want, and it's _not a coincidence_ if they calm down if the threat succeeds.

[needing to qualify words as our ontology changes
https://www.lesswrong.com/posts/wR4PaDp2Knu5coeXx/metaphorical-extensions-and-conceptual-figure-ground
https://unstableontology.com/2019/07/24/metaphors-and-conceptual-figure-ground-inversions/]




 
https://www.greaterwrong.com/posts/DSnamjnW7Ad8vEEKd/trivers-on-self-deception/comment/CandwLBdJXXq7Qxet


[can crimes be discussed literally
https://www.lesswrong.com/posts/N9oKuQKuf7yvCCtfq/can-crimes-be-discussed-literally ]

http://unremediatedgender.space/2016/Sep/psychology-is-about-invalidating-peoples-identities/

https://www.lesswrong.com/posts/i6fKszWY6gLZSX2Ey/fake-optimization-criteria


> "Accuse me of _fraud_? How _dare_ you?! Sure, I'm not a perfect person free from all bias, but—"

> "Bias. Is that your word for having a disposition to communicate in a way that causes others to make incorrect predictions about the value you have to offer, in a direction that moves resources towards you?"

> "Uh. I guess you could say that."

> "What do you think 'fraud' _is_, exactly?"


> among other things, the true referent of "consciousness" is also the cause in humans of talking about inner listeners.


https://www.lesswrong.com/posts/JoERzF8ePGr4zP9vv/self-deception-hypocrisy-or-akrasia


[conscious and unconscious fraud are different—but another agent who's dealing with you may wish to regard this as the difference between mergesort and quicksort]

https://www.lesswrong.com/posts/28bAMAxhoX3bwbAKC/are-your-enemies-innately-evil

https://sideways-view.com/2016/11/26/if-you-cant-lie-to-others-you-must-lie-to-yourself/

explain how: https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question
why do I _think_ the question is right: https://www.lesswrong.com/posts/rQEwySCcLtdKHkrHp/righting-a-wrong-question


> The bureaucrat, police officer, teacher, judge, or cable television company representative functions as [...], not as a co-modeling and fully interacting person. His behaviors are governed by top-down rules and scripts, with human discretion eliminated as much as possible.
>
> Sarah Perry, "The Essence of Peopling"

pretending to be stupid: https://slatestarcodex.com/2014/08/14/beware-isolated-demands-for-rigor/

also address: machine learning, deception in nature—it's _really convenient_ to use the "learning" and "deception" codewords, and there's no convenient replacement—maybe that suggests that they are the right words
