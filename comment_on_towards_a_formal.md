## Comment on "Toward a Formal Analysis of Deceptive Signaling"

[In this 2017 paper](http://philsci-archive.pitt.edu/13337/) (eventually [published in a 2019 volume of _Synthese_](https://link.springer.com/article/10.1007/s11229-017-1536-3)), Don Fallis and Peter J. Lewis grapple with potential flaws in previous work on how to formalize the concept of _deception_! The traditional account of deception is about _intentionally_ causing someone to have a _false belief_. If you don't end up with a false belief, then you clearly have not been deceived, but if no one _intended_ you to have that false belief—maybe someone who uttered a false thing to you just genuinely misspoke—then we don't want to say that that person has deceived you. But this definition has a few problems.

First, this "intentionally" business seems too anthropocentric if we want a concept of _deception_ that can cover what's going on with [mimickry, camoflage, "playing dead", fake alarm calls, _&c._ in nonhuman animals](https://en.wikipedia.org/wiki/Deception_in_animals), which don't necessarily have enough cognition involved for "intentionally" to apply. (And, I feel compelled to add, deception practiced by future artificial intelligences may have an alien shape to it that may go beyond merely human notions of "intent".)

Second, this idea of "false" "belief" is _so_ not Bayesian; we want to talk about updating probabilities in the _wrong direction_, without worrying about whether the probabilities cross some putative threshold between "belief" and "non-belief".

To put our analysis of truthful and deceptive communication on a more rigorous footing, we can study sender–receiver games, as pioneered by David Lewis, and further advanced in the work of Brian Skyrms. Two agents play a simple game: a sender observes one of several possible states of the world, and sends one of several possible signals. A receiver observes the signal, and chooses one of several possible actions. The agents get a reward (as specified in the game's payoff matrix) based on what state was observed by the sender and what action was chosen by the receiver. The meaning of a signal is how it changes the receiver's probabilities. If agents can learn to do things that get rewarded more (via reinforcement learning, or cultural evolution where more successful behaviors in a population get imitated more, whatever), then given the right payoff matrix, our agents evolve a signaling system where the sender chooses signals that the receiver interprets in a way that improves the receiver's choice of action. True information is communicated; the signals become a _shared_ map that reflect the territory.

This setting lets us generalize the "intending to cause a false belief" account of deception without the anthropmorphic black boxes of "intent" and "belief." Instead of someone causing someone else to have a false belief, we can talk about a sender causing a receiver to update in the wrong direction, and instead of someone _intending_ this, we can talk about the sender _benefiting_ from this (as quantified by the rewards in the game's payoff matrix).

One might ask: why require the sender to benefit in order for a signal to count as deceptive? Why isn't "made the receiver update in the wrong direction" enough?

The answer is that we're seeking an explanation for communication that _systematically_ makes receivers update in the wrong direction—signals that we can think of as having been _optimized for_ deception, rather that accidentally happening to do so on this particular occasion. The "rewards" (or, "payoffs", or "utility") in this model should be interpreted mechanistically, not necessarily mentalistically. It's _just_ that things that get "rewarded" more, happen more often. That's all—and that's enough to shape the evolution of how the system processes information. There need not any conscious mind that "feels happy" about getting rewarded (although that mechanism, among many alternatives, would do the trick).
