## Speaking Truth to Power Is a Schelling Point

Consider a coalition that wants to build accurate shared world-models (maps that reflect the territory), and then use those models to inform decisions that achieve the coalition's goals.

However, suppose that some ways of improving models are [punished by the surrounding Society](https://www.lesswrong.com/posts/DoPo4PDjgSySquHX8/heads-i-win-tails-never-heard-of-her-or-selective-reporting). For example, if [the Emperor's new clothes](https://en.wikipedia.org/wiki/The_Emperor%27s_New_Clothes) turn out to be ["vaporwear"](https://en.wikipedia.org/wiki/Vaporware), agents who notice this might not want to make it [common knowledge](https://www.lesswrong.com/posts/9QxnfMYccz9QRgZ5z/the-costly-coordination-mechanism-of-common-knowledge) within their coalition by adding it to the coalition's shared map, because if that knowledge "leaks" during the onerous process of applying for a grant from the Imperial Endowment for the Arts and Sciences, then the grant application will be more likely to be rejected: the Emperor's men don't want to fund coalitions who they can detect believe "negative" things about the Emperor, because those coalitions are more likely to be disloyal to the regime.

(Because while everyone has an interest in true beliefs, disloyal subjects have a unusually large interest in _selectively_ seeking out information that could be used against the regime during a revolution. ("The corrupt false Emperor is wasting your tax money on finery that _doesn't even exist!_ Will you join in our crusade?") That makes even true negative beliefs about the Emperor become a signal of disloyalty, which in turn gives loyal subjects an incentive to _avoid_ learning anything negative about the Emperor in order to credibly signal their loyalty.)

Coalitions need to model the world in order to achieve their goals, but grant money is useful, too. This scenario suggests coalition members working on their shared maps might follow a strategy schema that could be summarized in slogan form as—

> [Speak the truth, even if your voice trembles](https://www.lesswrong.com/posts/pZSpbxPrftSndTdSf/honesty-beyond-internal-truth)—_unless_ adding that truth to our map would make it ***x%*** harder for our coalition to compete for Imperial grant money, in which case, obfuscate, play dumb, [stonewall](https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters), [rationalize](https://www.lesswrong.com/posts/SFZoEBpLo9frSJGkc/rationalization), [report dishonestly](https://www.lesswrong.com/posts/XKGPgNLjPbjSAwLiL/dishonest-update-reporting), [filter evidence](https://www.lesswrong.com/posts/kJiPnaQPiy4p9Eqki/what-evidence-filtered-evidence), [violate](https://www.lesswrong.com/posts/GSz8SrKFfW7fJK2wN/relevance-norms-or-gricean-implicature-queers-the-decoupling) [Gricean maxims](https://en.wikipedia.org/wiki/Cooperative_principle#Grice's_maxims), [lie by omission](https://www.lesswrong.com/posts/PrXR66hQcaJXsgWsa/not-technically-lying), [gerrymander the relevant category boundaries](https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries), _&c._

(But [outright lying is out of the question](https://www.lesswrong.com/posts/MN4NRkMw7ggt9587K/firming-up-not-lying-around-its-edge-cases-is-less-broadly), because _that_ would be contrary to the moral law.)

Then the coalition faces a choice of the exact value of _x_. Smaller values of _x_ correspond to a more intellectually dishonest strategy, requiring only a small inconvenience before resorting to obfuscatory tactics. Larger values of _x_ correspond to more intellectual honesty: in the limit as _x_→∞, we just get, "Speak the truth, even if your voice trembles (full stop)."

Which choice of _x_ looks best is going to depend on the coalition's _current_ beliefs: coalition members can only deliberate on the optimal trade-off between map accuracy and money [using their _current_ map, rather than something else](https://www.lesswrong.com/posts/C8nEXTcjZb9oauTCW/where-recursive-justification-hits-bottom).

But [as the immortal Scott Alexander explains](https://www.lesswrong.com/posts/Kbm6QnJv9dgWsPHQP/schelling-fences-on-slippery-slopes), situations in which choices about the current value of a parameter, alter the process that makes future choices about that same parameter, are prone to a "slippery slope" effect: Gandhi isn't a murderer, but may quickly become one if he's willing to accept a bribe to take a pill that makes him both more violent and _less averse to taking more such pills_.

The slide down a slippery slope tends to stop at "sticky" Schelling points: choices that, for whatever reason, are unusually _salient_ in a way that makes them a natural focal point for mutual expectations, an answer different agents (or the same agent at different times) might give to the infinitely recursive question, "What would I do if I were her, wondering what she would do if she were me, wondering what ...?"

In the absence of distinguished salient intermediate points along the uniformly continuous trade-off between maximally accurate world-models and sucking up to the Emperor, the only Schelling points are _x_ = ∞ (tell the truth, the whole truth, and nothing but the truth) and _x_ = 0 (do everything short of outright lying to win grants). In this model, the tension between these two "attractors" for coordination may tend to promote coalitional schisms.
