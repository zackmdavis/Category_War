## Maybe Lying Doesn't Exist

In ["Against Lie Inflation"](https://slatestarcodex.com/2019/07/16/against-lie-inflation/), the immortal Scott Alexander argues that the word "lie" should be reserved for knowingly-made false statements, and not used in an expanded sense that includes unconscious motivated reasoning. Alexander argues that the expanded sense draws the category boundaries of "lying" too widely in a way that would make the word less useful. The hypothesis that predicts everything predicts nothing: in order for "Kevin lied" to _mean something_, some possible states-of-affairs need to be identified as _not_ lying, so that the statement "Kevin lied" can correspond to [redistributing conserved probability mass](http://yudkowsky.net/rational/technical/) _away from_ "not lying" states-of-affairs _onto_ "lying" states-of-affairs.

All of this is entirely correct. But Jessica Taylor (whose post ["The AI Timelines Scam"](https://unstableontology.com/2019/07/11/the-ai-timelines-scam/) inspired "Against Lie Inflation") wasn't arguing that _everything_ is lying; she was just using a _more_ permissive conception of lying than the one Alexander prefers, such that Alexander didn't think that Taylor's definition could stably and consistently identify non-lies.

Concerning Alexander's arguments against the expanded definition, I find I have one strong objection (that appeal-to-consequences is an invalid form of reasoning for optimal-categorization questions for essentially the same reason as it is for questions of simple fact), and one more speculative objection (that our intuitive "folk theory" of lying may actually be empirically mistaken). Let me explain.

(A small clarification: for myself, I notice that I _also_ tend to frown on the expanded sense of "lying". But the _reasons_ for frowning matter! People who superficially agree on a conclusion but for _different reasons_, are [not really on the same page](https://www.lesswrong.com/posts/n4ukoQzkgbAqpzqb5/argue-politics-with-your-best-friends)!)

### Appeals to Consequences Are Invalid

> There is no method of reasoning more common, and yet none more blamable, than, in philosophical disputes, to endeavor the refutation of any hypothesis, by a pretense of its dangerous consequences[.]
>
> —[David Hume](https://www.bartleby.com/37/3/12.html)

Alexander contrasts the imagined consequences of the expanded definition of "lying" becoming more widely accepted, to a world that uses the restricted definition:

> [E]veryone is much angrier. In the restricted-definition world, a few people write posts suggesting that there may be biases affecting the situation. In the expanded-definition world, those same people write posts accusing the other side of being liars perpetrating a fraud. I am willing to listen to people suggesting I might be biased, but if someone calls me a liar I'm going to be pretty angry and go into defensive mode. I'll be less likely to hear them out and adjust my beliefs, and more likely to try to attack them.

But this is an [appeal to consequences](https://en.wikipedia.org/wiki/Appeal_to_consequences). [Appeals to consequences](https://www.lesswrong.com/posts/P3FQNvnW8Cz42QBuA/dialogue-on-appeals-to-consequences) are invalid because they represent a map–territory confusion, an attempt to optimize our _description_ of reality at the expense of our ability to describe reality _accurately_ (which we need in order to _actually_ optimize reality).

(Again, the appeal is still invalid even if the conclusion—in this case, that unconscious rationalization shouldn't count as "lying"—might be true for _other reasons_.)

Some aspiring epistemic rationalists like to call this the ["Litany of Tarski"](https://wiki.lesswrong.com/wiki/Litany_of_Tarski). _If_ Elijah is lying (with respect to whatever the [optimal category boundary](https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries) for "lying" turns out to be according to [our standard Bayesian philosophy of language](https://www.lesswrong.com/posts/FaJaCgqBKphrDzDSj/37-ways-that-words-can-be-wrong)), _then_ I desire to believe that Elijah is lying (with respect to the optimal category boundary according to ... _&c._). _If_ Elijah is _not_ lying (with respect to ... _&c._), _then_ I desire to believe that Elijah is _not_ lying.

If the one comes to me and says, "Elijah is not lying; to support this claim, I offer this-and-such evidence of his sincerity," then this is right and proper, and I am eager to examine the evidence presented.

If the one comes to me and says, "You should choose to define _lying_ such that Elijah is not lying, because if you said that he was lying, then he might feel angry and defensive," this is _insane_. The map is not the territory! If Elijah's behavior is, _in fact_, deceptive—if he says things that cause people who trust him to be worse at [anticipating their experiences](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences) when he reasonably [could](https://www.lesswrong.com/posts/3buXtNiSK8gcRLMSG/possibility-and-could-ness) have avoided this—I can't make his behavior not-deceptive by _changing the meanings of words_.

Now, I _agree_ that it might very well empirically be the case that if I _say_ that Elijah is lying (where Elijah can hear me), he might get angry and defensive, which could have a variety of negative social consequences. But that's not an argument for changing the definition of lying; that's an argument that I have an incentive to lie about whether I think Elijah is lying! (Though [Glomarizing](https://www.lesswrong.com/posts/xdwbX9pFEr7Pomaxv/meta-honesty-firming-up-honesty-around-its-edge-cases#1__Glomarization_can_t_practically_cover_many_cases_) about whether I think he's lying might be an even better play.)

Alexander is concerned that people might strategically equivocate between different definitions of "lying" as an unjust social attack against the innocent, using the classic [motte-and-bailey](https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/) maneuver: first, argue that someone is "lying (expanded definition)" (the motte), then switch to treating them as if they were guilty of "lying (restricted definition)" (the bailey) and hope no one notices.

So, I agree that [this is a very real problem](https://www.lesswrong.com/posts/shoMpaoZypfkXv84Y/variable-question-fallacies). But it's worth noting that the problem of equivocation between [different category boundaries associated with the same word](https://www.lesswrong.com/posts/4FcxgdvdQP45D6Skg/disguised-queries) applies _symmetrically_: if it's possible to use an expanded definition of a socially-disapproved category as the motte and a restricted definition as the bailey in an unjust attack against the innocent, then it's _also_ possible to use an expanded definition as the bailey and a restricted definition as the motte in an unjust defense of the guilty. Alexander writes:

> The whole reason that rebranding lesser sins as "lying" is tempting is because everyone knows "lying" refers to something very bad. 

Right—and conversely, because everyone knows that "lying" refers to something very bad, it's tempting to rebrand lies as lesser sins. Ruby Bloom [explains what this looks like in the wild](https://www.lesswrong.com/posts/QB9eXzzQWBhq9YuB8/rationalizing-and-sitting-bolt-upright-in-alarm#R9kEwAz8YbQTWGPsB):

> I worked in a workplace where lying was commonplace, conscious, and system 2. Clients asking if we could do something were told "yes, we've already got that feature (we hadn't) and we already have several clients successfully using that (we hadn't)." Others were invited to be part an "existing beta program" _alongside others just like them_ (in fact, they would have been the very first). When I objected, I was told "no one wants to be the first, so you have to say that."
>
> [...] I think they lie to themselves that they're not lying (so that if you search their thoughts, they never think "I'm lying")[.]

If your interest in the philosophy of language is primarily to _avoid being blamed for things_—perhaps because [hard experience](https://slatestarcodex.com/2019/02/22/rip-culture-war-thread/) has taught you that you live [in a Hobbesian dystopia](https://www.lesswrong.com/posts/YRgMCXMbkKBZgMz4M/asymmetric-justice#puGDkhWCcaNJEMkdz) where the primary function of words is to elicit actions, where the [denotative structure](https://www.lesswrong.com/posts/i2bWqSFgyFxowTKWE/actors-and-scribes-words-and-deeds) of language was [eroded by political processes](https://www.lesswrong.com/posts/8XDZjfThxDxLvKWiM/excerpts-from-a-larger-discussion-about-simulacra) long ago, and all that's left is a [standardized list of approved attacks](https://www.lesswrong.com/posts/r2dTchodfqX4o5DYH/blame-games)—in that case, it makes perfect sense to worry about "lie inflation" but not about "lie deflation." If describing something as "lying" is primarily a weapon, then applying extra scrutiny to uses of that weapon is a wise arms-restriction treaty.

But if your interest in the philosophy of language is to improve and refine the uniquely human power of [vibratory telepathy](https://www.lesswrong.com/posts/SXK87NgEPszhWkvQm/mundane-magic)—to construct shared maps that reflect the territory—if you're interested in revealing what kinds of deception are _actually happening_, and why—

(in short, if you are an aspiring epistemic rationalist)

—then the asymmetrical fear of false-positive identifications of "lying" but not false-negatives—along with the focus on "bad actors", "stigmatization", "attacks", _&c._—just looks _weird_. What does _that_ have to do with maximizing the probability you assign to the right answer??

### The Optimal Categorization Depends on the Actual Psychology of Deception

> _Deception_  
> _My life seems like it's nothing but_  
> _Deception_  
> _A big charade_  
>
> _I never meant to lie to you_  
> _I swear it_  
> _I never meant to play those games_
>
> —["Deception"](https://www.youtube.com/watch?v=kQKs0eQHZRs) by Jem and the Holograms

Even if the fear of rhetorical warfare isn't a legitimate reason to avoid calling things lies (at least privately), we're still left with the main objection that "lying" is a _different thing_ from "rationalizing" or "being biased". Everyone is biased in some way or another, but to _lie_ is ["[t]o give false information intentionally with intent to deceive."](https://en.wiktionary.org/wiki/lie#Etymology_2) Sometimes it might make sense to use the word "lie" in a [noncentral](https://www.lesswrong.com/posts/yCWPkLi8wJvewPbEp/the-noncentral-fallacy-the-worst-argument-in-the-world) sense, as when we speak of "lying to oneself" or say "Oops, I lied" in reaction to being corrected. But it's important that these senses be explicitly acknowledged as noncentral and not conflated with the central case of knowingly speaking falsehood with intent to deceive—as Alexander says, conflating the two can only be to the benefit of _actual liars_.

Why would anyone disagree with this obvious ordinary view, if they _weren't_ trying to get away with the sneaky motte-and-bailey social attack that Alexander is so worried about?

Perhaps because the ordinary view relies an implied theory of human psychology that we have reason to believe is false? What if _conscious_ intent to deceive is typically absent in the most common cases of people saying things that (they would be capable of realizing upon being pressed) they know not to be true? Alexander writes—

> So how will people decide where to draw the line [if egregious motivated reasoning can count as "lying"]? My guess is: in a place drawn by bias and motivated reasoning, same way they decide everything else. The outgroup will be lying liars, and the ingroup will be decent people with ordinary human failings.

But if the word "lying" is to actually _mean something_ rather than just being a weapon, then the ingroup and the outgroup _can't both be right_. If [symmetry considerations](https://www.lesswrong.com/posts/28bAMAxhoX3bwbAKC/are-your-enemies-innately-evil) make us doubt that one group is really that much more honest than the other, that would seem to imply that _either_ both groups are composed of decent people with ordinary human failings, _or_ that both groups are composed of lying liars. The first description certainly _sounds nicer_, but as aspiring epistemic rationalists, we're _not allowed to care_ about which descriptions sound nice; we're _only_ allowed to care about which descriptions match reality.

And if all of the concepts available to us in our native language fail to match reality in different ways, then we have a tough problem that may require us to innovate.

The philosopher [Roderick T. Long writes](https://mises.org/library/rothbards-left-and-right-forty-years-later)—

> Suppose I were to invent a new word, "zaxlebax," and define it as "a metallic sphere, like the Washington Monument." That's the definition—"a metallic sphere, like the Washington Monument." In short, I build my ill-chosen example into the definition. Now some linguistic subgroup might start using the term "zaxlebax" as though it just meant "metallic sphere," or as though it just meant "something of the same kind as the Washington Monument." And that's fine. But my definition incorporates both, and thus conceals the false assumption that the Washington Monument is a metallic sphere; any attempt to use the term "zaxlebax," meaning what I mean by it, involves the user in this false assumption.

If self-deception is as ubiquitous in human life as authors such as [Robin Hanson](http://www.overcomingbias.com/tag/hypocrisy) argue (and if you're reading this blog, this should not be a new idea to you!), then the ordinary concept of "lying" may actually be analogous to Long's "zaxlebax": the standard [_intensional_ definition](https://www.lesswrong.com/posts/HsznWM9A7NiuGsp28/extensions-and-intensions) ("speaking falsehood with conscious intent to deceive"/"a metallic sphere") fails to match the most common extensional examples that we want to use the word for ("people motivatedly saying convenient things without bothering to check whether they're true"/"the Washington Monument").

Arguing for this _empirical_ thesis about human psychology is beyond the scope of this post. But _if_ we live in a sufficiently Hansonian world where the _ordinary_ meaning of "lying" fails to carve reality at the joints, then authors are faced with a tough choice: either be involved in the false assumptions of the standard believed-to-be-central intensional definition, or be deprived of the use of common [expressive vocabulary](https://www.lesswrong.com/posts/H7Rs8HqrwBDque8Ru/expressive-vocabulary). As Ben Hoffman [points out in the comments](https://slatestarcodex.com/2019/07/16/against-lie-inflation/#comment-777559) to "Against Lie Inflation", an earlier Scott Alexander didn't seem shy about calling people liars in his classic 2014 post ["In Favor of Niceness, Community, and Civilization"](https://slatestarcodex.com/2014/02/23/in-favor-of-niceness-community-and-civilization/)—

> Politicians lie, but not _too much_. Take the top story on Politifact Fact Check today. Some Republican claimed his supposedly-maverick Democratic opponent actually voted with Obama's economic policies 97 percent of the time. Fact Check explains that the statistic used was actually for all votes, not just economic votes, and that members of Congress typically have to have >90% agreement with their president because of the way partisan politics work. **So it's a lie, and is properly listed as one.** [bolding mine —ZMD] But it's a lie based on slightly misinterpreting a real statistic. He didn't just totally make up a number. He didn't even just make up something else, like "My opponent personally helped design most of Obama's legislation".

_Was_ the politician consciously lying? Or did he (or his staffer) arrive at the [misinterpretation via unconscious motivated reasoning](https://everythingstudies.com/2019/08/19/the-prince-and-the-figurehead/) and then just not bother to scrupulously check whether the interpretation was true? And how could Alexander know?

Given my current beliefs about the psychology of deception, I find myself inclined to reach for words like "motivated", "misleading", "distorted", _&c._, and am more likely to frown at uses of "lie", "fraud", "scam", _&c._ where intent is hard to establish. But even while frowning internally, I want to avoid [tone-policing](https://www.lesswrong.com/posts/bwkZD6uskCQBJDCeC/self-consciousness-wants-to-make-everything-about-itself#Tone_arguments) people whose word-choice procedures are calibrated differently from mine when I think I understand the structure-in-the-world they're trying to point to. Insisting on replacing the six instances of the phrase "malicious lies" in "Niceness, Community, and Civilization" with "maliciously-motivated false belief" would just be _worse writing_.

And I _definitely_ don't want to excuse motivated reasoning as a mere ordinary human failing for which someone can't be blamed! One of the key features that distinguishes motivated reasoning from simple mistakes is the way that the former _responds to incentives_ (such as being blamed). If the [elephant in your brain](http://elephantinthebrain.com/) thinks it can _get away with lying_ just by keeping conscious-you in the dark, it should think again!
