
with an $O(log n)$ binary search-like procedure

an $O(n)$ linear search


I'm _not_ talking about the problem of pressure on your beliefs

For example, if you said you've never stepped foot in Albuquerque, everyone would agree that this would be falsified by a (non-doctored) photo of you in Albuquerque, because "never stepped foot in" doesn't leave very much linguistic wiggle room.

But if you said you were "Fine" when I asked how you were, and actually you were a little bit sad, one could argue that it's not [...]


When's the last time you had motives to optimize your communication for having some affect on people other than causing them to have more accurate anticipations of experience? _Every fucking time you open your mouth_, that's when.

Suppose I see someone wearing a red shirt stealing from the cookie jar. Carol often wears red [maybe not the best example]

I will grant that not-lying has the advantage of being a bright line rather than a messy border

Bayesian 

on the other hand, you don't want to be exploitable by motivated misunderstandings

a topical example: OpenAI's Rubik's cube, Greg Brockman

Katja Grace on "principles are for bargaining"

disambiguate what I'm saying from "If we can't lie to others, we will lie to ourselves"

by "God" I just mean the order and beauty in the universe

Everybody knows that (lightning)

Atlas Shrugged State Science Institute

---------

Unilateralist's Blessing https://www.lesswrong.com/posts/8xomBzAcwZ6WTC8QB/steven0461-s-shortform-feed-1#P4J5jB3kmhFXMuRrM

, then authors would be faced with a tough choice:

[TODO: address stabilizing Schelling point objection]  
[TODO: maybe everyone _should_ be angrier, if lies are actualy that common and anger is a functional response?]  
[TODO: acknowledge that Alexander must know that the reason to "prefer" a category is _because_ it affects anticipations—"its web of connotations in our minds"]

https://www.lesswrong.com/posts/9QxnfMYccz9QRgZ5z/the-costly-coordination-mechanism-of-common-knowledge

ingroup/outgroup

(If you're reading this blog, this should not be a new idea to you!)

Ostrich World

This view is making some implicit assumptions about human psychology: 


As [Ben Hoffman points out in the comments](https://slatestarcodex.com/2019/07/16/against-lie-inflation/#comment-777559), 


> Politicians lie, but not _too much_. Take the top story on Politifact Fact Check today. Some Republican claimed his supposedly-maverick Democratic opponent actually voted with Obama's economic policies 97 percent of the time. Fact Check explains that the statistic used was actually for all votes, not just economic votes, and that members of Congress typically have to have >90% agreement with their president because of the way partisan politics work. **So it's a lie, and is properly listed as one.** But it's a lie based on slightly misinterpreting a real statistic. He didn't just totally make up a number. He didn't even just make up something else, like "My opponent personally helped design most of Obama's legislation". [bolding mine —ZMD]


https://slatestarcodex.com/2014/05/12/weak-men-are-superweapons/


> [P]eople think in terms of categories with central and noncentral members–a sparrow is a central bird, an ostrich a noncentral one. But if you live on the Ostrich World, which is inhabited only by ostriches, emus, and cassowaries, then probably an ostrich seems like a pretty central example of 'bird' and the first sparrow you see will be fantastically strange.

What if we live in Ostrich World? I mean—what if unconscious lying is the central case?

https://www.lesswrong.com/posts/pZSpbxPrftSndTdSf/honesty-beyond-internal-truth


dissemble


[TODO: reference http://benjaminrosshoffman.com/authenticity-vs-factual-accuracy/
http://benjaminrosshoffman.com/blackmailers-are-privateers-in-the-war-on-hypocrisy/
https://www.lesswrong.com/posts/bwkZD6uskCQBJDCeC/self-consciousness-wants-to-make-everything-about-itself
https://everythingstudies.com/2019/08/19/the-prince-and-the-figurehead/


https://slatestarcodex.com/2014/11/05/the-right-to-waive-your-rights/
https://slatestarcodex.com/2018/03/22/navigating-and-or-avoiding-the-inpatient-mental-health-system/
]

Robert S. Feldman, James A. Forrest, and Benjamin R. Happ




> Suppose I were to invent a new word, "zaxlebax," and define it as "a metallic sphere, like the Washington Monument." That's the definition—"a metallic sphere, like the Washington Monument." In short, I build my ill-chosen example into the definition. Now some linguistic subgroup might start using the term "zaxlebax" as though it just meant "metallic sphere," or as though it just meant "something of the same kind as the Washington Monument." And that's fine. But my definition incorporates both, and thus conceals the false assumption that the Washington Monument is a metallic sphere; any attempt to use the term "zaxlebax," meaning what I mean by it, involves the user in this false assumption.


----

DRAFT: "Maybe Lying Doesn't Exist"

Attention conservation notice: this is _not_ a "privileged" meeting of the Blight-alarmist coordination group (if that's a thing); this is just me personally sending _my_ blog post draft to a few people who I still happen to have outgoing-mail privileges with, for them to _probably_ ignore as they see fit.

Scott is too famous and too traumatized to do any real thinking these days, but the audience might learn something?

Writing the "Appeals to Consequences Are Invalid" section made me feel less angry at Scott over our earlier dispute, because I'm now leaning towards the hypothesis that he's _not_ opporunistically switching philosophy stances because throwing me under the bus helped him stay out of [Overton debt](https://www.lesswrong.com/posts/DoPo4PDjgSySquHX8/heads-i-win-tails-never-heard-of-her-or-selective-reporting)! He _actually doesn't get it_. He's _actually that dumb!_ (Trauma-induced dumb, not _g_-factor dumb.)

Hopefully the second section helps clarify the state of my persistent disagreement with the other Blight-alarmists on the lie/scam/fraud issue??

I originally went for "Appeals to Consequences Are Insane", but I'm going with "Invalid" as a strategic concession to the target audience, who will write me off as too "mean" if I say _insane_ in the section title. (One use of _insane_ survives in the text.)

Is it bad (is it lying??) if the title is kind of clickbaity? (Outrageous four-word summary incentivizes a click, but then the actual post is less outrageous.) Not sure what else I would call it.




> (in short, if you are an aspiring epistemic rationalist)

Author's note: this originally just read "if you are a rationalist", but 

---

Jessica explaining talking about conflict: https://www.greaterwrong.com/posts/9fB4gvoooNYa4t56S/power-buys-you-distance-from-the-crime/comment/S4QephDXJWqGJhuuH


----


[TODO: elaborate and explain that `reporter_2` at least _comes closer_ than reporter_1; the output still varies with reality; it's just distorted; whereas the liar was constant]

"Maybe Lying ..."

Three Algorithms of Deception

'My basic read of Zack's entire post was him saying over and over "Well there might be really bad instrumental effects of these arguments, but you have to ignore that if their epistemics are good." And my immediate reaction to that was "No I don't, and that's a bad norm."'

x₁

(I originally imagined this with normal distributions with different means, but choosing a discrete distribution makes explicit calculations easier, and it doesn't matter for the point I'm trying to make.)

ask mods whether it's possible to upload to images.lesswrong.com

[TODO: use "typical set" reasoning to show that these are distinct clusters
https://en.wikipedia.org/wiki/Asymptotic_equipartition_property
http://zackmdavis.net/blog/2019/05/the-typical-set/
]

[Leaky Generalizations](https://www.lesswrong.com/posts/Tc2H9KbKRjuDJ3WSS/leaky-generalizations)

Make sure to mention that clustering is hard—
http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/
http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/

Goodhart's Law: https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy regressional Goodhart

the intelligence, wisdom, maturity, civic-mindedness, perhaps motor skills (to fill in the ballot), _&c._ needed to exercise the franchise responsibly.

[important for game-theoretic reasons](https://www.lesswrong.com/posts/tJQsxD34maYw2g5E4/thomas-c-schelling-s-strategy-of-conflict#yqGXfQpsDuEGeL69a)

-------

_Author's Note on Contextualizing and the Development of Ideas_

[TODO: maybe a separate post???]

MacIver's book rec: https://twitter.com/DRMacIver/status/1139839288202297345

Imgur post: https://imgur.com/a/yjzfM9H

-------

Consider again the parable about the Sorter of bleggs and rubes who is at first overjoyed to be promoted to "Vice President of Sorting", only to be dismayed to learn that the change in title doesn't come with any material change in responsibilities or working conditions.

Is the new title a _lie_? I think many people would be inclined to say _No_, on the grounds that they expect 

[This](http://benjaminrosshoffman.com/excerpts-from-a-larger-discussion-about-simulacra/) may constitute a "security vulnerability" in [codes of ethics](https://www.lesswrong.com/posts/xdwbX9pFEr7Pomaxv/meta-honesty-firming-up-honesty-around-its-edge-cases) that have an [ethical injunction](https://www.lesswrong.com/posts/dWTEtgBfFaz6vjwQf/ethical-injunctions) against lying.


blue                   green
urban                  rural
indidividual taxes     merchant tax
strict marriage        easy divorce
Earth-centered         heliocenteric




An [evolutionary psychologist](TODO: linky) studying pair bonding may aspire to objectivity, but if she finds that

The Blues believe the Earth is a sphere at the center of the universe; the Greens believe the Earth is a plane that revolves around a sun.

https://slatestarcodex.com/2017/03/24/guided-by-the-beauty-of-our-weapons/
https://slatestarcodex.com/2013/12/29/the-spirit-of-the-first-amendment/

I'm usually an "Inside View" kind of guy, but _principles_ are an Outside View thing!

contrast—
https://twitter.com/ESYudkowsky/status/1170022426891018240
https://www.lesswrong.com/posts/k5qPoHFgjyxtvYsm7/stop-voting-for-nincompoops

if one side has all the brains

https://www.lesswrong.com/rationality/how-much-evidence-does-it-take

"Tails Risk"

the meta-hill is a Schelling point for dying on

What makes paying rent to the coalition bad, but having an obsessive special interest OK?

So, sure, I'd lie to a Nazi at my door asking me if I'm hiding any Jews. (Or about, um, being Jewish.) I also recommend lying to psychiatrists (who face incentives to [throw you in prison for 5.9 days](https://slatestarcodex.com/2018/03/22/navigating-and-or-avoiding-the-inpatient-mental-health-system/) in order to ensure [they don't get personally blamed for anything](https://slatestarcodex.com/2014/11/05/the-right-to-waive-your-rights/)).

> No exit—her shreds of awareness were saying, beating it into the pavements in the sound of her steps—no exit ... no refuge ... no signals ... no way to tell destruction from safety, or enemy from friend. ... Like that dog she had heard about, she thought ... somebody's dog in somebody's laboratory ... the dog who got his signals switched on him, and saw no way to tell satisfaction from torture, saw food changed to beatings and beatings to food, saw his eyes and ears deceiving him and his judgement futile and his consciousness impotent in a shifting, swimming, shapeless world—and gave up, refusing to eat at that price or live in a world of that kind. No!—was the only conscious word in her brain—no!—no!—no!—not your way, not your world—even if this "no" is all that's to be left of mine!
>
> —_Atlas Shrugged_ by Ayn Rand

Said on graphs: https://www.lesswrong.com/posts/y4bkJTtG3s5d6v36k/stupidity-and-dishonesty-explain-each-other-away?commentId=zcBFbHL2azWBSa4kY

Imgur post: https://imgur.com/a/aTcjknM

The answer is that we can apply our criteria for optimization, looking for theories that predict observed behavior that can be [specified using fewer bits](https://www.lesswrong.com/posts/f4txACqDWithRi7hs/occam-s-razor) by "looking backwards" from what goals the behavior achieves, rather than "looking forward" at the psychological mechanisms computing the behavior, which, in the absence [(as far as I know)](https://www.lesswrong.com/posts/vNBxmcHpnozjrJnJP/no-one-knows-what-science-doesn-t-know) of sufficiently advanced cognitive neuroscience, we can mostly only understand by the ["empathic inference"](https://www.lesswrong.com/posts/9fpWoXpNv83BAHJdc/the-comedy-of-behaviorism) of imagining what we would do if we were the subject, using our own brain as a "black box" to predict others. [TODO: simplify!!]


> among other things, the true referent of "consciousness" is also the cause in humans of talking about inner listeners.


https://www.lesswrong.com/posts/JoERzF8ePGr4zP9vv/self-deception-hypocrisy-or-akrasia


[conscious and unconscious fraud are different—but another agent who's dealing with you may wish to regard this as the difference between mergesort and quicksort]

https://www.lesswrong.com/posts/28bAMAxhoX3bwbAKC/are-your-enemies-innately-evil

https://sideways-view.com/2016/11/26/if-you-cant-lie-to-others-you-must-lie-to-yourself/

explain how: https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question
why do I _think_ the question is right: https://www.lesswrong.com/posts/rQEwySCcLtdKHkrHp/righting-a-wrong-question


> The bureaucrat, police officer, teacher, judge, or cable television company representative functions as [...], not as a co-modeling and fully interacting person. His behaviors are governed by top-down rules and scripts, with human discretion eliminated as much as possible.
>
> Sarah Perry, "The Essence of Peopling"

pretending to be stupid: https://slatestarcodex.com/2014/08/14/beware-isolated-demands-for-rigor/

also address: machine learning, deception in nature—it's _really convenient_ to use the "learning" and "deception" codewords, and there's no convenient replacement—maybe that suggests that they are the right words

https://www.greaterwrong.com/posts/DSnamjnW7Ad8vEEKd/trivers-on-self-deception/comment/CandwLBdJXXq7Qxet


not satisfied with quasi- https://www.lesswrong.com/posts/FT9Lkoyd5DcCoPMYQ/partial-summary-of-debate-with-benquo-and-jessicata-pt-1?commentId=coWFfoYqdeuSPpTqe#vPekZcouSruiCco3c

[can crimes be discussed literally
https://www.lesswrong.com/posts/N9oKuQKuf7yvCCtfq/can-crimes-be-discussed-literally ]

> "Accuse me of _fraud_? How _dare_ you?! Sure, I'm not a perfect person free from all bias, but—"
>
> "Bias. Is that your word for 'having a disposition to communicate in a way that causes others to make incorrect predictions about the value you have to offer, in a direction that moves resources towards you'?"
>
> "Uh. I guess you could say that."
>
> "What do you think 'fraud' _is_, exactly?"

----

There's no rule of rationality against _lying_.

https://arbital.greaterwrong.com/p/executable_philosophy?l=112

what that _means_ is that (at some appropriate level of abstraction) there's a little [Bayesian network](https://www.lesswrong.com/posts/hzuSDMx7pd2uxFc5w/causal-diagrams-and-causal-models) in my head with "blueness" and "eggness" observation nodes hooked up to a central "blegg" category-membership node, such that if I see a black-and-white photograph of an egg-shaped object, I can use the observation of its shape to update my beliefs about its blegg-category-membership, and then use my beliefs about category-membership to update my beliefs about its blueness. This cognitive algorithm is useful if we live in a world where objects that have the appropriate statistical structure—if the joint distribution P(blegg, blueness, eggness) approximately factorizes as P(blegg)·P(blueness|blegg)·P(eggness|blegg).


"Category boundaries" are just a _visual metaphor_ for the math: the set of things I'll classify as a blegg with probability greater than _p_ is conveniently _visualized_ as an area with a boundary in blueness–eggness space. If you _don't understand_ the relevant math and philosophy—or are pretending not to understand only and exactly when it's politically convenient—you might think you can redraw the boundary any way you want, but you can't, because the "boundary" visualization is _derived from_ a statistical model which corresponds to _empirically testable predictions about the real world_. Fucking with category boundaries corresponds to fucking with the model, which corresponds to fucking with your ability to interpret sensory data.

 The only two reasons you could _possibly_ want to do this would be to wirehead yourself (corrupt your map to make the territory look nicer than it really is, making yourself _feel_ happier at the cost of sabotaging your ability to navigate the real world) or as information warfare (corrupt shared maps to sabotage other agents' ability to navigate the real world, in a way such that you benefit from their confusion).

https://getpocket.com/explore/item/is-lab-grown-meat-really-meat

utility of certainty

(not even one unit of epistemology, I feel like my epistemology will let me make exactly as good predictions as yours will about any [...] in the vicinity)

> You _can_ define a word any way you want, you just have to pay the costs [later, when saying you can still do Bayesian inference on the gerrymandered category]


But if another system _does_ need to make inferences about an object's features, 



https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace

Suppose there's a robot arm in the Sorting room that puts bleggs in the blegg bin, which gets taken to the vanadium-ore processing room, where a more sophisticated  vanadium-ore processing machinery elsewhere in the factory that needs to handle both bleggs and gretrahedrons.

The ore-processor's models might be different from the three-feature models we used to _identify_ bleggs in the Sorting room—maybe it needs to vary its drill speed in proportion to the density of a particular blegg's flexible outer material.


Maybe the robot arm that puts bleggs in the blegg bin doesn't need to _know_ about the blueness and eggness scores: it can close its claws around rubes and bleggs alike, and you only need to program it to drop an object into the correct bin when told that the object "is a blegg" or "is a rube".

But if other machines in the factory need to do something more complicated—if they need to make _further inferences_ about something already known to be in the "vanadium-ore-containing blue eggs" cluster, it's a more efficient communication protocol to 


is a _specific mathematical model_ that makes _specific_ (probabilistic) predictions. What it _means_ is that if we see a black-and-white photo of an egg-shaped object (specifically, one with an eggness score of 7)

Scott on "voluntary"




 do cognitive work concerning our sortable objects, category labels can be used as signals to link up the models between different systems. Suppose there's some delicate vanadium-ore processing machinery elsewhere in the factory that needs to handle both bleggs and gretrahedrons. You want to be able to send commands to that machine, telling it to process a `BLEGG` using its _own_ models, _without_ having to

send over all the binary code of the Bayesian network and feature extractors that we used to identify the blegg.

The ore-processor's models might be different from the three-feature models we used to identify bleggs in the Sorting room—maybe it needs to vary its drill speed in proportion to the density of a particular blegg's flexible outer material.

[another diagram about using a signal to link up different models]

[there are more facts about bleggs than just three]

https://www.foodsafety.gov/food-safety-charts/safe-minimum-cooking-temperature

https://www.greaterwrong.com/users/zack_m_davis?offset=360

[argument from common usage]

There's simply _no such thing as lying_ if you're allowed to arbitrarily redefine words and claim without evidence that ["everybody knows"](https://www.lesswrong.com/posts/BNfL58ijGawgpkh9b/everybody-knows) what you mean!

If you want to lie, you might be able to get away with it! There's [no God to prevent it](TODO: linky "Beyond the Reach of God")!

"But she still did it because she valued that choice above others—because of the feeling of importance she attached to that decision."
https://www.lesswrong.com/posts/n5ucT5ZbPdhfGNLtP/terminal-values-and-instrumental-values

[Do I need to define "communication signal" somewhere?]

[TODO Example: wireheading—believing I'm attractive]



[TODO Example: butterfly mimickry]

[TODO Example: is artificial meat "real meat"]


https://www.lesswrong.com/posts/zNcLnqHF5rvrTsQJx/zut-allais


I think a fundamental misunderstanding of what the "utility function" concept is good for is masking the real reason we're having this argument.

I'm reminded of the claim that everyone is intrinsically selfish—that we only do things because of their effects on our own state of mind. [When confronted with the counterexample of a mother sacrificing her life to save her child, the reply goes: "But she still did it because _she valued_ that choice above others—because of the _feeling of importance_ she attached to that decision."](https://www.lesswrong.com/posts/n5ucT5ZbPdhfGNLtP/terminal-values-and-instrumental-values)

  

In [the von Neumann and Morgenstern formalism](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem), agents make choices between _lotteries_: probability distributions over outcomes. Given a few hard-to-deny axioms of what sane decisionmaking should look like, we can prove that agents' choices correspond to maximizing some utility function over _outcomes_. _Not_ over lotteries.

The phenomenon that the lottery formalism captures is that when you take some action or execute some plan in the real world, you don't know what will happen with certainty; rather, you have some probability distribution over what will happen as a result of your actions.


and you want to choose the action with the best result in expectation. Actions are justified in terms of _expected_ utility: the sum of the utilities of the possible outcomes weighted by probability. Inside the formalism, it doesn't make sense to talk about preferences for taking an action as something distinct from that action's distribution of consequences, because the formalism describes _how to compute_ what action to take given a model of the world (expressible as a map from actions to probability distributions) and given your preferences (expressible as a utility function). [TODO: use "plan" as a synonym for action]

When applying the formalism to real life, preferences should be folded into the "outcomes".

Suppose I'm deciding whether to run a mile to the store to buy soap, or order the soap online.

[TODO: this is a misunderstanding of what "utility" is. "Because of the feeling of importance she attached to that decision". vNM showed that their axioms plus preferences over lotteries imply behaving as if maximizing a utility function. It's not that an agent assigns utility to choosing this and such lottery; it's just that if it's behaving coherently, it has to act as if assigned utilities to the _outcomes within_ the lottery. Similarly, it would be weird and vacuous to assign utility to choosing a particular communication system, but we can look at what info the system is optimized to convey
 "the utility of certainty" https://www.lesswrong.com/posts/zNcLnqHF5rvrTsQJx/zut-allais

]

[TODO Example: "safe" meat temperature. If we start out with a discrete distribution between 100F and 200F. If I tell you the temperature is between 165 and 200, I've cut down your uncertainty from lg(100)=6.643 to lg(35)=5.1292: 1.514 bits, because I cut down the number of possibilities by a factor of 2.85, and lg(2.85)=1.514. But if I told you the temperature was _either_ between 165 and 190, OR between 130 and 140, that's ALSO cutting it down to 35 possibilities, but it doesn't answer the question I want to know about, which is whether I'll get sick from eating. Objection: but isn't that "instrumental"? It "safe" depends on your values! Reply: no, it's a conditional prediction about bacteria and getting sick.]


[TODO: Objection: what if wireheading is good? Don't we like candy, even if it could be construed as "deceiving" our nutrition detectors as they were designed? Reply: maybe you can get away with wireheading the "taste" system if you have some _other_ system making sure you make sane nutrition.]

[TODO: decision-determined problems]
