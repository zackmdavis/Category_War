# Reasons to Go On

_(Content warning: discussion of suicide, suffering, cosmic horror.)_

> _So wish again  
> The sun is smiling  
> But always above you  
> The idea raises its head_
>
> —Laura Barrett, "Deception Island Optimists Club"

Is life worth living?

I mean, in general. _My_ life has been worth living—so far, on net. _My_ life looks fine on the timescale of years or decades. But I seem _unusually_ lucky compared to the the rest of the universe I can see: I'm young _and_ smart _and_ rich _and_ healthy _and_ free during [an absurd dreamtime when our economy has been growing faster than population for 200 years](https://www.overcomingbias.com/2009/09/this-is-the-dream-time.html). Most animals in the history of life on Earth have not had the advantages I enjoy—[and there are various reasons why the future might not be good](https://centerforreducingsuffering.org/intro/).

When I look back at _the worst_ moments of my life—the pain during a salivary stone flare-up in 2008, the Hell-depths of my 2013 and 2017 psychotic breaks—_those_ moments were definitely _not_ worth living. If _most_ of the universe is relevantly _like that_ (due to Malthusianism, the intelligence explosion going poorly, or any other reason), then the universe shouldn't exist.

I've accumulated a bunch of arguments for the "normie" position that the _decision-theoretically relevant_ answer is, "Yes, life is worth living", but I can never quite manage to put my weight down on them; it still seems like a meaningfully open question. I doubt anyone has much reassurance (or decision-theoretically relevant anti-reassurance) that I haven't already thought of, but I might as well write down what I have on the robot-cult blog in case it helps anyone else. (I think the arguments have helped me _relative_ to my worst fears.)

### Decision-Theoretic Relevance

In a way, the question is kind of _retarded_. From an AI-design perspective, the reason to think things are good or bad is in order to compute decisions that "steer" the universe "towards" good things and "away from" bad things.

* decision theoretic relevance: if my local environment is fine, I can work on optimizing _that_, and treat everything else as part of the "generalized past"
* ethical to have children, if the world becomes worse?
* if s-risks are real, then I have the opporunity to reduce them, right?

### General Considerations

* it looks good because there's not much reason for anti-altruists to exist
* smarter agents should fight less because they can predict
* evolution asymmetry: fitness per second, and hiding the keys
* evolution doesn't care about whether reward or punishment does the job, but agents try to grab their own reward buttons, so the universe should contain more reward than punishment
* the worst experience in my life is worse than the worse experience in an ant's life; the big picture is what matters, more than my intuition that crucifixion couldn't _really_ happen
* I don't fear death; I only fear all ancestry-relevant _causes_ of death

### More Specific Considerations

* the time I wrote a chess engine, and it made bad moves—sign-flip seems unlikely, but I'm unhappy that it's not _zero_— https://github.com/zackmdavis/Leafline/commit/ec8a4c26bff61ea485dbd407b1bced24e20e5b85
* flawed Singularity risk—even if it won't be "With Folded Hands" or "I Have No Mouth" _specifically_, the analogue of a buggy GAN might still be bad, analogously to psychotic-break badness
* there's no bit you can flip on a human to turn them evil: realistic AGI systems are more like a human rather than the sort-by-minimax-score function in my chess engine—except
* game-theory bullshit
* the "hard takeoff" mindset rather than a Christiano-like view makes it seem less worth trying—
* an aligned AI could reduce suffering elsewhere: rescue simulations, wildlife in our future life cone, acausal game-theory bullshit
* I don't take quantum immortality seriously; obviously you need to weight by measure
