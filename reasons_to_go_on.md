# Reasons to Go On

_(Content warning: discussion of suicide, suffering, cosmic horror.)_

> _So wish again  
> The sun is smiling  
> But always above you  
> The idea raises its head_
>
> —Laura Barrett, "Deception Island Optimists Club"

Is life worth living?

I mean, in general. _My_ life has been worth living—so far, on net. _My_ life looks fine on the timescale of years or decades. But I seem _unusually_ lucky compared to the the rest of the universe I can see: I'm young _and_ smart _and_ rich _and_ healthy _and_ free during [an absurd dreamtime when our economy has been growing faster than population for 200 years](https://www.overcomingbias.com/2009/09/this-is-the-dream-time.html). Most animals in the history of life on Earth have not had the advantages I enjoy—[and there are various reasons why the future might not be good](https://centerforreducingsuffering.org/intro/).

When I look back at _the worst_ moments of my life—the pain during a salivary stone flare-up in 2008, the Hell-depths of my 2013 and 2017 psychotic breaks—_those_ moments were definitely _not_ worth living. If _most_ of the universe is relevantly _like that_ (due to Malthusianism, the intelligence explosion going poorly, or any other reason), then the universe shouldn't exist.

I've accumulated a bunch of arguments for the "normie" position that the _decision-theoretically relevant_ answer is, "Yes, life is worth living", but I can never quite manage to put my weight down on them; it still seems like a meaningfully open question. I doubt anyone has much reassurance (or decision-theoretically relevant anti-reassurance) that I haven't already thought of, but I might as well write down what I have on the robot-cult blog in case it helps anyone else. (I think the arguments have helped me _relative_ to my worst fears.)

### Decision-Theoretic Relevance

In a way, the question is kind of _retarded_. From an AI-design perspective, the reason to think things are good or bad is in order to compute decisions that "steer" the universe "towards" good things and "away from" bad things.

* decision theoretic relevance: if my local environment is fine, I can work on optimizing _that_, and treat everything else as part of the "generalized past"
* ethical to have children, if the world becomes worse?
* if s-risks are real, then I have the opporunity to reduce them, right?

### General Considerations

* it looks good because there's not much reason for anti-altruists to exist
* smarter agents should fight less because they can predict
* evolution asymmetry: fitness per second, and hiding the keys
* evolution doesn't care about whether reward or punishment does the job, but agents try to grab their own reward buttons, so the universe should contain more reward than punishment
* the worst experience in my life is worse than the worse experience in an ant's life; the big picture is what matters, more than my intuition that crucifixion couldn't _really_ happen
* I don't fear death; I only fear all ancestry-relevant _causes_ of death

### More Specific Considerations

* the time I wrote a chess engine, and it made bad moves—sign-flip seems unlikely, but I'm unhappy that it's not _zero_— https://github.com/zackmdavis/Leafline/commit/ec8a4c26bff61ea485dbd407b1bced24e20e5b85
* flawed Singularity risk—even if it won't be "With Folded Hands" or "I Have No Mouth" _specifically_, the analogue of a buggy GAN might still be bad, analogously to psychotic-break badness
* there's no bit you can flip on a human to turn them evil: realistic AGI systems are more like a human rather than the sort-by-minimax-score function in my chess engine—except
* game-theory bullshit
* the "hard takeoff" mindset rather than a Christiano-like view makes it seem less worth trying, but 
* an aligned AI could reduce suffering elsewhere: rescue simulations, wildlife in our future life cone, acausal game-theory bullshit
* I don't take quantum immortality seriously; obviously you need to weight by measure
* Anthropomorphic pessimism: people imagine evolution/AI doing nice things, because they can't imagine an optimization process that they can't model via empathic inference. Your brain is a high-ranking (relative to your preference-ordering) solution generator, so the default is to mentally focus on high-ranking solutions (for you) and selectively search for arguments—whereas actually, evolution or a paperclip maximizer wouldn't even formulate those considerations to begin with. Some negative-leaning utilitarians have a mental illness that gives them the opposite problem. Your brain is also built to be afraid of bad things happening, so it's easy to hype oneself into being scared and selectively searching for reasons that other agents might want to hurt you. But "You can't get more paperclips that way" applies both ways (you can't get more paperclips as a weird side-effect of being good, compared to just manufacturing paperclips the ordinary way, but you also can't get more paperclips as a weird side-effect of being evil, for the same reason—trade/conflict/extortion are the _only_ reasons you would even compute good and evil as an intermediary step in paperclip planning—and superintelligences don't fight wars)
* Consequentialists who want someone to do a thing are roughly indifferent between, "Extort them with threat cost C to do the thing", "Pay them C to do the thing", "Steal it from them, with expenses C"
* If you have a communication channel, no reason to credibly extort when you can get what you want cheaper with deception/manipulation; accurate simulation is a communication channel; the fact that you imagine threats but don't imagine superhuman manipulation/deception shows that you're doing anthropomorphic pessimism rather than simulation
* This is the security vulnerability why religions feature stories about Hell: memes that promise punishment (even completely made-up punishments) create incentives to propagate themselves among dumb apes that aren't sure that the punishments are made-up
* Reading Paul Christiano's consequentialists-in-the-universal-prior stuff makes me less scared, because he's a mathematician rather than a cultist, so I walk away with the takeaway of "This is weird and confusing; I'm not smart enough to understand it", rather than tapping the security vulnerability; it's an incredibly technical argument
* "acausal" isn't actually special (it's the limiting case of making deals with no communication whatsoever, but that's on a spectrum with lesser or greater amounts of communication: "only get a hint, and have to infer" vs. "detailed contract negotiation" (even contract negotiations depend on _performing inferences_ about communication signals, even if you're not consciously aware of the inference step)
* most spam is promises (I am a Nigerian prince) rather than threats (this is the FBI)—and is fake in both cases (there's no Nigerian money or jail sentence in the real world; it's just spam that someone has an incentive to create)
* "defeating Dr. Evil" is a contrived situation; if I got a threat in my actual email as a human, I would assume, "Someone who knows I'm a mentally ill Less Wrong reader is trying to mess with me" and delete it, becuase I live in a world where humans can't be copied. (Even if this is a simulation, it's a faithful simulation of the basement.) If we lived in the world of Age of Em, that would be a different situation—but I don't live in the world of Age of Em (which we don't think will happen because neuromorphic AI would come first—even if neuromorphic AIs are similar to ems in some ways).
* different kinds of darkness (innoculation)
* some convergent interest in simulating history (to learn about what happened); even if I'm grateful to be an American, the thought of punishing Loyalists in the American revolutionary war seems silly
* if you did steal someone's source code in Age of Em, you'd steal their password out of their brain rather than blackmailing
* Why isn't obeying the government blackmail? Well, the difference between blackmail and trade is "where you set the zero point" of what you would have done in the absence of interaction. The government is doing you a favor by letting you pay taxes voluntarily rather than the counterfactual of having to bring out the police to take your gold (which is more expensive for both you and the government). But why is that the relevant counterfactual—why is the government in charge? Something about Schelling points—individual citizens can't coordinate to overthrow the regime
* uFAIs in general aren't on the same team: paperclip maximizers and staple maximizers don't like each other. The strategy-stealing assumption will hold for superintelligences (even if it doesn't hold among agents in different "weight classes"): we expect FAIs and paperclip maximizers and staple maximizers to find a Pareto-efficient deal with each other (FAIs being rare isn't a trade barrier to them), not fighting—violence is the last refuge of the incompetent
 * Negative consideration: a system that gets punished if you scream has an incentive to remove your mouth ("hopefully" if things have gone that wrong, it also deletes the part of your brain that screams, so you're just dead rather than suffering and unable to communicate)—but what exact failure is likely is a critical question: if it modifies you into _wanting_ to press the reward function, that's better than taking control of your arm while you horrifiedly look on
* in a way, Christiano's "unaligned benchmark" is scarier; I'd rather be killed than taken care of by something with unaligned me-regarding preferences (school and hospitals are already bad along this dimension; what if they had nanotech?—but this also applies to "mundane" transhumanist scenarios, somewhat)

I was listening to music, and being scared of—what if I couldn't turn the music off? But, this generalizes—most energies, temperatures, pressures, chemical compositioins, &c. are not survivable; my brain generated that specific case to focus on

In one of the 80K podcasts, Christiano speculates that a paperclip maximizer might be the kind of thing you could only get from "clean" alignment-attempt AI, because of the analogy between evolution and gradient descent

The distribution of AIs we end up with isn't actually a random utility function https://arbital.com/p/random_utility_function/

* How hard is "altruism" and "person-affecting views" to discover? I think I want to be nice to aliens that "already exist", rather than eating their atoms for what I think is optimal. (Or maybe, eat their atoms for an optimal computing substrate, but compensate them by having some of the compute go to their CEV.) If evolution+culture could cough up that motive into me, could gradient-descent+training-data cough something similar into AI?
* Altruism without person-affecting views is slightly incoherent (I want to help aliens that already exist, but I wouldn't invent aliens from scratch)

* How "unnatural" is altruism? If we wanted to benefit to octupuses or dolphins in their own "moral reference frame", we could probably figure how to do it (and be conservative where we didn't know), rather than tiling the universe with tiny happy-dolphin sculptures

* Even without AI, technology/evolution would destroy human nature anyway, just more slowly. Is IDA/reward-modeling going off the rails, worse than that?

* Anthropics: finding yourself on ancient Earth could be because the future is more _diverse_?—there are _more_ types of UFAI, many of which have an incentive to simulate ancestral civilizations. Bad news because of the Anna K. principle?
