(I continue to maintain that this is fun and basic hidden-Bayesian-structure-of-language-and-cognition stuff that _shouldn't_ be "political", but the ["Containment Thread on the Motivation and Political Context for My Philosophy of Language Agenda"](https://www.lesswrong.com/posts/5aqumaym7Jd2qhDcy/containment-thread-on-the-motivation-and-political-context) is now available for talking about the elephant in the room if we need it.)

(I intend to _eventually_ reply to all substantive critical comments on this post and the containment thread, but I have [other things to focus on this month](TODO: linky math and wellness month) (despite wanting to publish this now so that it can double as a review of ["Where to Draw the Boundaries?"](https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries) for the [2019 Review](https://www.lesswrong.com/posts/QFBEjjAvT6KbaA3dY/the-lesswrong-2019-review) before the review phase ends on 11 January), so many of those replies may be kicked out to February; your patience is deeply appreciated.)


-------

[motte-and-bailey is wrong because of the math of equivocation, not because it's for a good cause; local validity]

that anyone who liked the Sequences might enjoy and all Friendliness researchers should know instinctively and no one should feel politically threatened by (a


 but if anyone wants to interrogate me about the elephant in the room, let's take it to the 



—although for the context of where I'm coming from, I would recommend reading ["Sexual Dimorphism in Yudkowsky's Sequences, in Relation to My Gender Problems"]() first (it's "only", um, [TODO: real wordcount] 20,000 words), and it might take me a few days to reply because I have a dayjob)


(If anyone wants to make this political, please take it to the ["Containment Thread on the Motivation and Political Context for My Philosophy of Language Agenda"](https://www.lesswrong.com/posts/5aqumaym7Jd2qhDcy/containment-thread-on-the-motivation-and-political-context); it may take me a few days to reply to comments, but I definitely will)



-----



(If anyone wants to talk about the elephant in the room, feel free to do so in the , but I continue to maintain that the correctness of my philosophy-of-language agenda can be evaluated in an elephant-independent way)





There's no rule of rationality against _lying_.

https://arbital.greaterwrong.com/p/executable_philosophy?l=112

what that _means_ is that (at some appropriate level of abstraction) there's a little [Bayesian network](https://www.lesswrong.com/posts/hzuSDMx7pd2uxFc5w/causal-diagrams-and-causal-models) in my head with "blueness" and "eggness" observation nodes hooked up to a central "blegg" category-membership node, such that if I see a black-and-white photograph of an egg-shaped object, I can use the observation of its shape to update my beliefs about its blegg-category-membership, and then use my beliefs about category-membership to update my beliefs about its blueness. This cognitive algorithm is useful if we live in a world where objects that have the appropriate statistical structure—if the joint distribution P(blegg, blueness, eggness) approximately factorizes as P(blegg)·P(blueness|blegg)·P(eggness|blegg).


"Category boundaries" are just a _visual metaphor_ for the math: the set of things I'll classify as a blegg with probability greater than _p_ is conveniently _visualized_ as an area with a boundary in blueness–eggness space. If you _don't understand_ the relevant math and philosophy—or are pretending not to understand only and exactly when it's politically convenient—you might think you can redraw the boundary any way you want, but you can't, because the "boundary" visualization is _derived from_ a statistical model which corresponds to _empirically testable predictions about the real world_. Fucking with category boundaries corresponds to fucking with the model, which corresponds to fucking with your ability to interpret sensory data.

 The only two reasons you could _possibly_ want to do this would be to wirehead yourself (corrupt your map to make the territory look nicer than it really is, making yourself _feel_ happier at the cost of sabotaging your ability to navigate the real world) or as information warfare (corrupt shared maps to sabotage other agents' ability to navigate the real world, in a way such that you benefit from their confusion).

https://getpocket.com/explore/item/is-lab-grown-meat-really-meat

utility of certainty

(not even one unit of epistemology, I feel like my epistemology will let me make exactly as good predictions as yours will about any [...] in the vicinity)

> You _can_ define a word any way you want, you just have to pay the costs [later, when saying you can still do Bayesian inference on the gerrymandered category]


But if another system _does_ need to make inferences about an object's features, 



https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace

Suppose there's a robot arm in the Sorting room that puts bleggs in the blegg bin, which gets taken to the vanadium-ore processing room, where a more sophisticated  vanadium-ore processing machinery elsewhere in the factory that needs to handle both bleggs and gretrahedrons.

The ore-processor's models might be different from the three-feature models we used to _identify_ bleggs in the Sorting room—maybe it needs to vary its drill speed in proportion to the density of a particular blegg's flexible outer material.


Maybe the robot arm that puts bleggs in the blegg bin doesn't need to _know_ about the blueness and eggness scores: it can close its claws around rubes and bleggs alike, and you only need to program it to drop an object into the correct bin when told that the object "is a blegg" or "is a rube".

But if other machines in the factory need to do something more complicated—if they need to make _further inferences_ about something already known to be in the "vanadium-ore-containing blue eggs" cluster, it's a more efficient communication protocol to 


is a _specific mathematical model_ that makes _specific_ (probabilistic) predictions. What it _means_ is that if we see a black-and-white photo of an egg-shaped object (specifically, one with an eggness score of 7)

Scott on "voluntary"




 do cognitive work concerning our sortable objects, category labels can be used as signals to link up the models between different systems. Suppose there's some delicate vanadium-ore processing machinery elsewhere in the factory that needs to handle both bleggs and gretrahedrons. You want to be able to send commands to that machine, telling it to process a `BLEGG` using its _own_ models, _without_ having to

send over all the binary code of the Bayesian network and feature extractors that we used to identify the blegg.

The ore-processor's models might be different from the three-feature models we used to identify bleggs in the Sorting room—maybe it needs to vary its drill speed in proportion to the density of a particular blegg's flexible outer material.

[another diagram about using a signal to link up different models]

[there are more facts about bleggs than just three]

https://www.foodsafety.gov/food-safety-charts/safe-minimum-cooking-temperature

https://www.greaterwrong.com/users/zack_m_davis?offset=360

[argument from common usage]

There's simply _no such thing as lying_ if you're allowed to arbitrarily redefine words and claim without evidence that ["everybody knows"](https://www.lesswrong.com/posts/BNfL58ijGawgpkh9b/everybody-knows) what you mean!

If you want to lie, you might be able to get away with it! There's [no God to prevent it](TODO: linky "Beyond the Reach of God")!

"But she still did it because she valued that choice above others—because of the feeling of importance she attached to that decision."
https://www.lesswrong.com/posts/n5ucT5ZbPdhfGNLtP/terminal-values-and-instrumental-values

[Do I need to define "communication signal" somewhere?]

[TODO Example: wireheading—believing I'm attractive]



[TODO Example: butterfly mimickry]

[TODO Example: is artificial meat "real meat"]


https://www.lesswrong.com/posts/zNcLnqHF5rvrTsQJx/zut-allais


I think a fundamental misunderstanding of what the "utility function" concept is good for is masking the real reason we're having this argument.

I'm reminded of the claim that everyone is intrinsically selfish—that we only do things because of their effects on our own state of mind. [When confronted with the counterexample of a mother sacrificing her life to save her child, the reply goes: "But she still did it because _she valued_ that choice above others—because of the _feeling of importance_ she attached to that decision."](https://www.lesswrong.com/posts/n5ucT5ZbPdhfGNLtP/terminal-values-and-instrumental-values)

  

In [the von Neumann and Morgenstern formalism](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem), agents make choices between _lotteries_: probability distributions over outcomes. Given a few hard-to-deny axioms of what sane decisionmaking should look like, we can prove that agents' choices correspond to maximizing some utility function over _outcomes_. _Not_ over lotteries.

The phenomenon that the lottery formalism captures is that when you take some action or execute some plan in the real world, you don't know what will happen with certainty; rather, you have some probability distribution over what will happen as a result of your actions.


and you want to choose the action with the best result in expectation. Actions are justified in terms of _expected_ utility: the sum of the utilities of the possible outcomes weighted by probability. Inside the formalism, it doesn't make sense to talk about preferences for taking an action as something distinct from that action's distribution of consequences, because the formalism describes _how to compute_ what action to take given a model of the world (expressible as a map from actions to probability distributions) and given your preferences (expressible as a utility function). [TODO: use "plan" as a synonym for action]

When applying the formalism to real life, preferences should be folded into the "outcomes".

[TODO: at some point, I need to address and cite Fallis/Lewis vs. Skyrms on the definition of deception—I found Fallis/Lewis's paper convincing (that Skyrms's definition is too broad), but when developing my thesis, Skryms's definition feels more natural, which is either a point for Skryms, or maybe my sense is actually compatible with Fallis/Lewis]

Suppose I'm deciding whether to run a mile to the store to buy soap, or order the soap online.

[TODO: this is a misunderstanding of what "utility" is. "Because of the feeling of importance she attached to that decision". vNM showed that their axioms plus preferences over lotteries imply behaving as if maximizing a utility function. It's not that an agent assigns utility to choosing this and such lottery; it's just that if it's behaving coherently, it has to act as if assigned utilities to the _outcomes within_ the lottery. Similarly, it would be weird and vacuous to assign utility to choosing a particular communication system, but we can look at what info the system is optimized to convey
 "the utility of certainty" https://www.lesswrong.com/posts/zNcLnqHF5rvrTsQJx/zut-allais

]

[TODO Example: "safe" meat temperature. If we start out with a discrete distribution between 100F and 200F. If I tell you the temperature is between 165 and 200, I've cut down your uncertainty from lg(100)=6.643 to lg(35)=5.1292: 1.514 bits, because I cut down the number of possibilities by a factor of 2.85, and lg(2.85)=1.514. But if I told you the temperature was _either_ between 165 and 190, OR between 130 and 140, that's ALSO cutting it down to 35 possibilities, but it doesn't answer the question I want to know about, which is whether I'll get sick from eating. Objection: but isn't that "instrumental"? It "safe" depends on your values! Reply: no, it's a conditional prediction about bacteria and getting sick.]


[TODO: Objection: what if wireheading is good? Don't we like candy, even if it could be construed as "deceiving" our nutrition detectors as they were designed? Reply: maybe you can get away with wireheading the "taste" system if you have some _other_ system making sure you make sane nutrition.]




-----

Suppose that the word "attractive" gets applied to people who are 80th-percentile-or-above in the trait of attractiveness. (Of course, actual language use is not so quantitatively precise, but we continue the methodology of using simple numerical examples to help us understand the structure of cognition.)

Suppose I want to believe that I'm attractive.

[TODO: What if calling someone unattractive makes them sad?! Reply: decision-determined problems, optimization coming from upstream]

[Objection: but isn't wireheading good? Candy is "deceiving" your taste-nutrition system. Reply: maybe, but you'd better make damned sure you don't need that decision to make decsions]

----

[TODO section: Skyrms and his critics on deception]


"Explicitly And With Public Focus On The Language And Its Meaning" (deception is impossible in equilibrium, but there's no reason to try to move to a less efficient equilibrium unless you're trying to profit outside of equilibrium)


I want to claim there's an isomorphism between "lying x% of the time" and "defining a new category system such that x% of the data is misclassified with respect to the old system." But it's not _exactly_ an isomorphism



----

If there's any difference between the two worlds, it must be in the minds of the participants.


if you _expected_ me to tell the truth with respect to your definition of 'gold', then you would feel betrayed when I didn't, but if you already _knew_ that I define 'gold' differently, then you would not feel betrayed.


If there's no court to create common knowledge as to which meaning is 'correct' (such that those trying to profit by misusing the signal can be punished for fraud), then in equilibrium, a buyer who doesn't care about serial numbers also has no reason to care whether my definition is correct, or whether gold 'really' means yellowish-atomic-number-79-metal but people lie about it a third of the time.


If we assume that the factory doesn't need to make any decisions based on the blueness or eggness scores of rubes or unclassified objects, then as far as the _inputs and outputs_ between the supplier and the various machines in the factory are concerned, there's _no difference_ between adopting the gerrymandered _blegg\*_ category and keeping the old categories but just _lying_ that 3 rubes out of every 16[^lying-frequencies-rube] and 7 unclassified objects out of every 64[^lying-frequencies-unclassified] are bleggs!

[^lying-frequencies-rube]: [TODO]

[^lying-frequencies-unclassified]: The [TODO]

And this Orwellian _mind game_ of redefining words in unnatural ways, confident that you'll be able to dodge any accusations of dishonesty with the excuse of, "Oh, but I'm not lying about the location of points in configuration space—I can still tell you the blueness or eggness score of any individual object if you ask about that—I'm just chopping up the space into an alternate category system that better satisfies my goals, and reusing existing words for my new categories", is just a really sophisticated way of deceiving people without having to admit to yourself that you're deceiving people!

I'll admit, it's clever! You are very smart! And it's _true_ that words don't have intrinsic definitions! And that the same word can be used with different meanings that can be inferred from context! And that there are many ways to chop up configuration space into categories depending on your goals—you might want to run your clustering algorithm with respect to different subspaces or with different weights on the axes depending on exactly what variables you want your category to predict!

However! It's _also_ true that there are mathematical laws describing the relationship between the categories you attach to your communication signals, and what the agents receiving those signals are able to infer, such that some categories are better-performing cognitive technology than others! The "I'm not lying about the location of point in configuration space" excuse is _nonsense_ because language relies on inductive inference; an omniscient being that somehow knew and could communicate the exact location of a point in the space [would have no need of category labels](https://www.lesswrong.com/posts/i2dfY65JciebF3CAo/empty-labels). So if there's _already_ a equilibrium where agents are using an _existing_ signal to make decision-relevant predictions, then sending that signal under conditions that correspond to a different and worse-performing category is going to cause agents that receive the signal to make worse predictions and therefore worse decisions! So you're deceiving them! Because that's what _deception_ means!

It's not wrong for the supplier to _want_ to make more money by supplying more bleggs. But to do that _legitimately_, they need to _actually come up with more bleggs in the territory_. Scribbling new definitions on the map to pass off rubes as bleggs is just committing fraud!

[_Moral Mazes_](https://www.lesswrong.com/posts/45mNHCMaZgsvfDXbw/quotes-from-moral-mazes) quotes an executive: "We lie all the time, but if everyone knows that we're lying, is a lie really a lie?" The man has a point.

the "meaning" of a signal is just how it changes a receiver's probabilities. If you’ve read the Sequences, this should be familiar theme: "the true import of a thing is its entanglement with other things".

(Observations that trigger dolphin-reports from me are a _tiny_ subset of the space of possible observations.)

Intelligent systems with shared interests will design communication protocols to efficiently encode information in accordance with the [_mathematical laws_](https://www.lesswrong.com/posts/eY45uCCX7DdwJ4Jha/no-one-can-exempt-you-from-rationality-s-laws) of probability and information theory. Systems that communicate in ways that deviate from efficient encodings, do so in order to achieve non-shared interests (via deception), possibly non-shared interests of subsystems _within_ an agent (wireheading).

drawing a map that inaccurately represents

https://www.lesswrong.com/posts/bTsiPnFndZeqTnWpu/mixed-reference-the-great-reductionist-project

Suppose that, in common usage, the word "attractive" gets applied to people who are 80th-percentile-or-above in the underlying continuous trait of attractiveness. (Of course, actual language use is not so quantitatively precise, but we continue to follow the methodology of using simple numerical examples to help us understand the structure of cognition.)

Suppose I want to be attractive, but I'm only at the 47th percentile of attractiveness. Figuring out how to increase my attractiveness looks like _hard work_: it might require such ruinously expensive measures like going to the gym, buying new clothes, or addressing deep-seated personality flaws. It would be a lot easier to just _change my definition of the word "attractive"_ to mean being above the 46th percentile in attractiveness. Sure, the _status quo_ is for people to use that word to mean being above the 80th percentile in attractivenesss, but words don't have intrinsic meanings, and



So, this obviously doesn't work. (Okay, it "works" if you deliberately choose to define the word "work" such that it works, but it doesn't _actually_ work.) The motion to redefine the word "attractive" came with the purported justification that words don't have intrinsic meanings, so it can't be "wrong" to redefine it.


 Rather, I mentally associate the word "attractive" with being above the 80th percentile in attractiveness. By using that word with a _different_ meaning (being above the 46th percentile) and then [_failing to keep track_](https://www.lesswrong.com/posts/shoMpaoZypfkXv84Y/variable-question-fallacies) of the fact that _attractive(46+)_ and _attractive(80+)_ have _different meanings_, I might feel better about myself.

In political discourse, this kind of strategic equivocation is sometimes called [the motte and bailey doctrine](https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/): [TODO: it's still bad math even when you're being nice to someone]

You can't make the world better by changing your _description_ of the world; that's wireheading. Rather, you want your description to match reality, so that you can use it to help you _figure out_ how to make the world better.


----

Jessica wrote:

> Say we have a distribution P over R^n.  Let f : R^n -> C be a categorization function, where C is a finite set of categories.

> The speaker will observe X (the R^n value) and tell the listener c = f(X).  The listener's distribution over X is now P(X | f(X) = c).  The (multivariate) variance of this distribution equals E[||X - E[X | f(X) = c]||^2] which is the listener's expected square error (once the listener knows the categorization)

> If we take the expectation of this quantity with c being a random variable distributed as P(f(X)), we get the listener's overall expected square error (with the expectation being from the point of view of not knowing the categorization, but knowing that the listener will know the categorization).  This provides a way of scoring the categorization function f.

> This is well-motivated because there are real-world problems where guessing numeric quantities is necessary and getting closer is better, e.g. buying a shirt for someone that matches their size.  Bits that help guess these quantities accurately are more valuable than bits that don't help guess these quantities.

even EY's clarfication talks about "redrawing social or epistemic boundaries"—maybe need a separate section to address why social categories need to flow through epistemic categories with respect to some space—I think the age-of-majority (or doctor or figherfighter) is again my go-to example here; language has to refer to a cluster in social behavior, although the cluster in social behavior may get shaped by language

https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Bias%E2%80%93variance_decomposition_of_mean_squared_error
https://en.wikipedia.org/wiki/Mean_squared_error


https://www.lesswrong.com/posts/hAvGi9YAPZAnnjZNY/prediction-compression-transcript-1
https://www.lesswrong.com/posts/ex63DPisEjomutkCw/msg-len
https://www.lesswrong.com/posts/mB95aqTSJLNR9YyjH/message-length
https://www.lesswrong.com/posts/tWLFWAndSZSYN6rPB/think-like-reality

More links—
https://www.lesswrong.com/posts/9QxnfMYccz9QRgZ5z/the-costly-coordination-mechanism-of-common-knowledge#Coordination_Problems
https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no
https://www.lesswrong.com/posts/sP2Hg6uPwpfp3jZJN/lost-purposes




This is also, at an abstract high level, how human natural language works—how it _has_ to work. There is an idea in my head. When I speak, my larynx creates pressure wave in the air. When the waves hit your eardrum and are decoded by your auditory cortex, that hopefully [triggers](https://www.lesswrong.com/posts/YF9HB6cWCJrDK5pBM/words-as-mental-paintbrush-handles) a similar idea in your head. You don't have direct access to the computations going on in my head, but your brain can _infer_ something about them from the noises I make, and that's what it means to speak and to listen in a reductionist universe.

I am not a cognitive scientist and I don't _know_ the details of _how_ language works in the brain. But I do know a _little_ information theory and probability theory—enough to glimpse a bit of how the laws of mathematics constain how language _has_ to work, to the extent that it works.

In studying or explaining the math, I like to focus on simple examples with explicit probability distributions that I can do my own calculations for with pencil or Python. If I want to tell a story to go along with the math, I want to make the story about factory machines that I could actually program.

The actual implementation of natural language in human brains is going to be _much_ more complicated, of course. Telling a story problem about computer programs controlling factory machines has the advantage not only of being a simple explanation of the [math](https://www.lesswrong.com/posts/bkSkRwo9SRYxJMiSY/beautiful-probability) that we can [trust](https://www.lesswrong.com/posts/BL9DuE2iTCkrnuYzx/trust-in-bayes) governs the more complicated real-world phenomenon. It's also less tempting to rationalize about the story problem about factory machines, than it is to directly think about language.

(As it is [written of the tenth virtue](https://yudkowsky.net/rational/virtues/) of precision: even if you cannot do the math, knowing that the math exists tells you that the dance step is precise and has no room in it for your whims.)

Humans are designed to decieve each other—it's always tempting to speak in a way that propagates misinformation while retaining deniability that we weren't _lying_—it's the other guy's _fault_ for misinterpreting what I _really meant_. When we think about designing messages for computer programs to give commands to each other about quantifiable observables, this excuse vanishes: if there's a bug in deterministic computer code such that the robot arm puts an object in the rube bin when it gets the `BLEGG` message, then that's what happens. There's no room to use the complexity of humans and their political games to obscure the behavior of the physical system and how it's processing information.

[TODO: more cleanly distinguish two reasons for example choice: simplicity, and sidestepping political intuitions. Link to "Toolbox-thinking and law-thinking": https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking
]

[TODO: rewrite to introduce the conflict more neutrally? Mention eggs as well as meat?]

[ https://slate.com/technology/2018/07/should-lab-grown-meat-be-called-meat.html ]
[... vegan meat]

[TODO
factory farming is an ongoing moral catastrophe
fortunately, substitutes are being developed
unfortunately, the substitutes aren't being adopted
right now, there's no real conflict; people accept veggie burger as a "marked" term, with "veggie" as a privative adjective, just as they accept toy guns
animal rights activists might embark on a language-reform crusade that cultured meat, or plant-based meat _is_ meat, in the hopes of shaping demand
if the substitutes are good enough, no problem—in our world, we don't care (although we could imagine a hunter-gatherer society that had a relationship with the Hunt for whom it wasn't the same)
if the substitutes aren't good enough, meat-eaters regard this as aggression and mind control
the vegan mind-control crowd has a real utility argument—the animals are in fact suffering and dying, but it's not cause-neutral
the cognitive function of categorization is part of the common interest of many causes: I say "nonhuman animals" not because I'm a vegan, but because it's actually accurate
]



Is artificial meat, real meat?

There is a _trivial_ sense in which a smart-aleck could answer, It depends on how you draw the category boundaries of _meat_, [end-of-discussion](https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters).

But when such [_failures to consider the obvious next question_](https://www.lesswrong.com/posts/FWMfQKG3RpZx6irjm/semantic-stopsigns) are dragged into prominence by [half-rationalists](), the result is _not_ rationality, nor till the rationalists among us can be above insolence and triviality and

pink margerine




https://poets.org/poem/poetry



billions of animals are born into miserable conditions and live lives of unbearable suffering, all so that the flesh-eating monsters that call themselves "humans" can enjoy [feasting on their corpses](https://reducing-suffering.org/how-much-direct-suffering-is-caused-by-various-animal-foods/).

(Bear with me—I'm saying this to set up a [conflict](https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/) scenario for making a point about the philosophy of language; you should be able to follow the upcoming thought experiment even if your own views on morality and nonhuman animal consciousness don't consider factory farming a moral catastrophe.)


If we could get the sam

[TODO: symmetry of ag interests bashing good mimics and animal interests bashing bad mimics]

"Cultured" meat (grown from animal cells without the whole animal, rather than made from plants) is in its infancy, yet agricultural interests have [already made attempts to prevent it from being labeled and sold as _meat_](https://slate.com/technology/2018/07/should-lab-grown-meat-be-called-meat.html), for the obvious financial reasons. 

On the other side, we could imagine animal-rights activists lobbying for language norms that insist that even unrealistic meat substitutes _are_ meat, because of all the suffering on factory farms that would be averted if that convention were to be upheld. Even if the most die-hard carnivores were to cling to their ways as best they could (paying the extra three syllables of saying "_animal_ meat" every time, after Society at large decided that )

