
There's no rule of rationality against _lying_.

https://arbital.greaterwrong.com/p/executable_philosophy?l=112

what that _means_ is that (at some appropriate level of abstraction) there's a little [Bayesian network](https://www.lesswrong.com/posts/hzuSDMx7pd2uxFc5w/causal-diagrams-and-causal-models) in my head with "blueness" and "eggness" observation nodes hooked up to a central "blegg" category-membership node, such that if I see a black-and-white photograph of an egg-shaped object, I can use the observation of its shape to update my beliefs about its blegg-category-membership, and then use my beliefs about category-membership to update my beliefs about its blueness. This cognitive algorithm is useful if we live in a world where objects that have the appropriate statistical structure—if the joint distribution P(blegg, blueness, eggness) approximately factorizes as P(blegg)·P(blueness|blegg)·P(eggness|blegg).


"Category boundaries" are just a _visual metaphor_ for the math: the set of things I'll classify as a blegg with probability greater than _p_ is conveniently _visualized_ as an area with a boundary in blueness–eggness space. If you _don't understand_ the relevant math and philosophy—or are pretending not to understand only and exactly when it's politically convenient—you might think you can redraw the boundary any way you want, but you can't, because the "boundary" visualization is _derived from_ a statistical model which corresponds to _empirically testable predictions about the real world_. Fucking with category boundaries corresponds to fucking with the model, which corresponds to fucking with your ability to interpret sensory data.

 The only two reasons you could _possibly_ want to do this would be to wirehead yourself (corrupt your map to make the territory look nicer than it really is, making yourself _feel_ happier at the cost of sabotaging your ability to navigate the real world) or as information warfare (corrupt shared maps to sabotage other agents' ability to navigate the real world, in a way such that you benefit from their confusion).

https://getpocket.com/explore/item/is-lab-grown-meat-really-meat

utility of certainty

(not even one unit of epistemology, I feel like my epistemology will let me make exactly as good predictions as yours will about any [...] in the vicinity)

> You _can_ define a word any way you want, you just have to pay the costs [later, when saying you can still do Bayesian inference on the gerrymandered category]


But if another system _does_ need to make inferences about an object's features, 



https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace

Suppose there's a robot arm in the Sorting room that puts bleggs in the blegg bin, which gets taken to the vanadium-ore processing room, where a more sophisticated  vanadium-ore processing machinery elsewhere in the factory that needs to handle both bleggs and gretrahedrons.

The ore-processor's models might be different from the three-feature models we used to _identify_ bleggs in the Sorting room—maybe it needs to vary its drill speed in proportion to the density of a particular blegg's flexible outer material.


Maybe the robot arm that puts bleggs in the blegg bin doesn't need to _know_ about the blueness and eggness scores: it can close its claws around rubes and bleggs alike, and you only need to program it to drop an object into the correct bin when told that the object "is a blegg" or "is a rube".

But if other machines in the factory need to do something more complicated—if they need to make _further inferences_ about something already known to be in the "vanadium-ore-containing blue eggs" cluster, it's a more efficient communication protocol to 


is a _specific mathematical model_ that makes _specific_ (probabilistic) predictions. What it _means_ is that if we see a black-and-white photo of an egg-shaped object (specifically, one with an eggness score of 7)

Scott on "voluntary"




 do cognitive work concerning our sortable objects, category labels can be used as signals to link up the models between different systems. Suppose there's some delicate vanadium-ore processing machinery elsewhere in the factory that needs to handle both bleggs and gretrahedrons. You want to be able to send commands to that machine, telling it to process a `BLEGG` using its _own_ models, _without_ having to

send over all the binary code of the Bayesian network and feature extractors that we used to identify the blegg.

The ore-processor's models might be different from the three-feature models we used to identify bleggs in the Sorting room—maybe it needs to vary its drill speed in proportion to the density of a particular blegg's flexible outer material.

[another diagram about using a signal to link up different models]

[there are more facts about bleggs than just three]

https://www.foodsafety.gov/food-safety-charts/safe-minimum-cooking-temperature

https://www.greaterwrong.com/users/zack_m_davis?offset=360

[argument from common usage]

There's simply _no such thing as lying_ if you're allowed to arbitrarily redefine words and claim without evidence that ["everybody knows"](https://www.lesswrong.com/posts/BNfL58ijGawgpkh9b/everybody-knows) what you mean!

If you want to lie, you might be able to get away with it! There's [no God to prevent it](TODO: linky "Beyond the Reach of God")!

"But she still did it because she valued that choice above others—because of the feeling of importance she attached to that decision."
https://www.lesswrong.com/posts/n5ucT5ZbPdhfGNLtP/terminal-values-and-instrumental-values

[Do I need to define "communication signal" somewhere?]

[TODO Example: wireheading—believing I'm attractive]



[TODO Example: butterfly mimickry]

[TODO Example: is artificial meat "real meat"]


https://www.lesswrong.com/posts/zNcLnqHF5rvrTsQJx/zut-allais


I think a fundamental misunderstanding of what the "utility function" concept is good for is masking the real reason we're having this argument.

I'm reminded of the claim that everyone is intrinsically selfish—that we only do things because of their effects on our own state of mind. [When confronted with the counterexample of a mother sacrificing her life to save her child, the reply goes: "But she still did it because _she valued_ that choice above others—because of the _feeling of importance_ she attached to that decision."](https://www.lesswrong.com/posts/n5ucT5ZbPdhfGNLtP/terminal-values-and-instrumental-values)

  

In [the von Neumann and Morgenstern formalism](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem), agents make choices between _lotteries_: probability distributions over outcomes. Given a few hard-to-deny axioms of what sane decisionmaking should look like, we can prove that agents' choices correspond to maximizing some utility function over _outcomes_. _Not_ over lotteries.

The phenomenon that the lottery formalism captures is that when you take some action or execute some plan in the real world, you don't know what will happen with certainty; rather, you have some probability distribution over what will happen as a result of your actions.


and you want to choose the action with the best result in expectation. Actions are justified in terms of _expected_ utility: the sum of the utilities of the possible outcomes weighted by probability. Inside the formalism, it doesn't make sense to talk about preferences for taking an action as something distinct from that action's distribution of consequences, because the formalism describes _how to compute_ what action to take given a model of the world (expressible as a map from actions to probability distributions) and given your preferences (expressible as a utility function). [TODO: use "plan" as a synonym for action]

When applying the formalism to real life, preferences should be folded into the "outcomes".

[TODO: at some point, I need to address and cite Fallis/Lewis vs. Skyrms on the definition of deception—I found Fallis/Lewis's paper convincing (that Skyrms's definition is too broad), but when developing my thesis, Skryms's definition feels more natural, which is either a point for Skryms, or maybe my sense is actually compatible with Fallis/Lewis]

Suppose I'm deciding whether to run a mile to the store to buy soap, or order the soap online.

[TODO: this is a misunderstanding of what "utility" is. "Because of the feeling of importance she attached to that decision". vNM showed that their axioms plus preferences over lotteries imply behaving as if maximizing a utility function. It's not that an agent assigns utility to choosing this and such lottery; it's just that if it's behaving coherently, it has to act as if assigned utilities to the _outcomes within_ the lottery. Similarly, it would be weird and vacuous to assign utility to choosing a particular communication system, but we can look at what info the system is optimized to convey
 "the utility of certainty" https://www.lesswrong.com/posts/zNcLnqHF5rvrTsQJx/zut-allais

]

[TODO Example: "safe" meat temperature. If we start out with a discrete distribution between 100F and 200F. If I tell you the temperature is between 165 and 200, I've cut down your uncertainty from lg(100)=6.643 to lg(35)=5.1292: 1.514 bits, because I cut down the number of possibilities by a factor of 2.85, and lg(2.85)=1.514. But if I told you the temperature was _either_ between 165 and 190, OR between 130 and 140, that's ALSO cutting it down to 35 possibilities, but it doesn't answer the question I want to know about, which is whether I'll get sick from eating. Objection: but isn't that "instrumental"? It "safe" depends on your values! Reply: no, it's a conditional prediction about bacteria and getting sick.]


[TODO: Objection: what if wireheading is good? Don't we like candy, even if it could be construed as "deceiving" our nutrition detectors as they were designed? Reply: maybe you can get away with wireheading the "taste" system if you have some _other_ system making sure you make sane nutrition.]




-----

Suppose that the word "attractive" gets applied to people who are 80th-percentile-or-above in the trait of attractiveness. (Of course, actual language use is not so quantitatively precise, but we continue the methodology of using simple numerical examples to help us understand the structure of cognition.)

Suppose I want to believe that I'm attractive.

[TODO: What if calling someone unattractive makes them sad?! Reply: decision-determined problems, optimization coming from upstream]

[Objection: but isn't wireheading good? Candy is "deceiving" your taste-nutrition system. Reply: maybe, but you'd better make damned sure you don't need that decision to make decsions]

----

[TODO section: Skyrms and his critics on deception]


"Explicitly And With Public Focus On The Language And Its Meaning" (deception is impossible in equilibrium, but there's no reason to try to move to a less efficient equilibrium unless you're trying to profit outside of equilibrium)


I want to claim there's an isomorphism between "lying x% of the time" and "defining a new category system such that x% of the data is misclassified with respect to the old system." But it's not _exactly_ an isomorphism



----

If there's any difference between the two worlds, it must be in the minds of the participants.


if you _expected_ me to tell the truth with respect to your definition of 'gold', then you would feel betrayed when I didn't, but if you already _knew_ that I define 'gold' differently, then you would not feel betrayed.


If there's no court to create common knowledge as to which meaning is 'correct' (such that those trying to profit by misusing the signal can be punished for fraud), then in equilibrium, a buyer who doesn't care about serial numbers also has no reason to care whether my definition is correct, or whether gold 'really' means yellowish-atomic-number-79-metal but people lie about it a third of the time.


If we assume that the factory doesn't need to make any decisions based on the blueness or eggness scores of rubes or unclassified objects, then as far as the _inputs and outputs_ between the supplier and the various machines in the factory are concerned, there's _no difference_ between adopting the gerrymandered _blegg\*_ category and keeping the old categories but just _lying_ that 3 rubes out of every 16[^lying-frequencies-rube] and 7 unclassified objects out of every 64[^lying-frequencies-unclassified] are bleggs!

[^lying-frequencies-rube]: [TODO]

[^lying-frequencies-unclassified]: The [TODO]

And this Orwellian _mind game_ of redefining words in unnatural ways, confident that you'll be able to dodge any accusations of dishonesty with the excuse of, "Oh, but I'm not lying about the location of points in configuration space—I can still tell you the blueness or eggness score of any individual object if you ask about that—I'm just chopping up the space into an alternate category system that better satisfies my goals, and reusing existing words for my new categories", is just a really sophisticated way of deceiving people without having to admit to yourself that you're deceiving people!

I'll admit, it's clever! You are very smart! And it's _true_ that words don't have intrinsic definitions! And that the same word can be used with different meanings that can be inferred from context! And that there are many ways to chop up configuration space into categories depending on your goals—you might want to run your clustering algorithm with respect to different subspaces or with different weights on the axes depending on exactly what variables you want your category to predict!

However! It's _also_ true that there are mathematical laws describing the relationship between the categories you attach to your communication signals, and what the agents receiving those signals are able to infer, such that some categories are better-performing cognitive technology than others! The "I'm not lying about the location of point in configuration space" excuse is _nonsense_ because language relies on inductive inference; an omniscient being that somehow knew and could communicate the exact location of a point in the space [would have no need of category labels](https://www.lesswrong.com/posts/i2dfY65JciebF3CAo/empty-labels). So if there's _already_ a equilibrium where agents are using an _existing_ signal to make decision-relevant predictions, then sending that signal under conditions that correspond to a different and worse-performing category is going to cause agents that receive the signal to make worse predictions and therefore worse decisions! So you're deceiving them! Because that's what _deception_ means!

It's not wrong for the supplier to _want_ to make more money by supplying more bleggs. But to do that _legitimately_, they need to _actually come up with more bleggs in the territory_. Scribbling new definitions on the map to pass off rubes as bleggs is just committing fraud!

[_Moral Mazes_](https://www.lesswrong.com/posts/45mNHCMaZgsvfDXbw/quotes-from-moral-mazes) quotes an executive: "We lie all the time, but if everyone knows that we're lying, is a lie really a lie?" The man has a point.

the "meaning" of a signal is just how it changes a receiver's probabilities. If you’ve read the Sequences, this should be familiar theme: "the true import of a thing is its entanglement with other things".

(Observations that trigger dolphin-reports from me are a _tiny_ subset of the space of possible observations.)

Intelligent systems with shared interests will design communication protocols to efficiently encode information in accordance with the [_mathematical laws_](https://www.lesswrong.com/posts/eY45uCCX7DdwJ4Jha/no-one-can-exempt-you-from-rationality-s-laws) of probability and information theory. Systems that communicate in ways that deviate from efficient encodings, do so in order to achieve non-shared interests (via deception), possibly non-shared interests of subsystems _within_ an agent (wireheading).

drawing a map that inaccurately represents

https://www.lesswrong.com/posts/bTsiPnFndZeqTnWpu/mixed-reference-the-great-reductionist-project

Suppose that, in common usage, the word "attractive" gets applied to people who are 80th-percentile-or-above in the underlying continuous trait of attractiveness. (Of course, actual language use is not so quantitatively precise, but we continue to follow the methodology of using simple numerical examples to help us understand the structure of cognition.)

Suppose I want to be attractive, but I'm only at the 47th percentile of attractiveness. Figuring out how to increase my attractiveness looks like _hard work_: it might require such ruinously expensive measures like going to the gym, buying new clothes, or addressing deep-seated personality flaws. It would be a lot easier to just _change my definition of the word "attractive"_ to mean being above the 46th percentile in attractiveness. Sure, the _status quo_ is for people to use that word to mean being above the 80th percentile in attractivenesss, but words don't have intrinsic meanings, and



So, this obviously doesn't work. (Okay, it "works" if you deliberately choose to define the word "work" such that it works, but it doesn't _actually_ work.) The motion to redefine the word "attractive" came with the purported justification that words don't have intrinsic meanings, so it can't be "wrong" to redefine it.


 Rather, I mentally associate the word "attractive" with being above the 80th percentile in attractiveness. By using that word with a _different_ meaning (being above the 46th percentile) and then [_failing to keep track_](https://www.lesswrong.com/posts/shoMpaoZypfkXv84Y/variable-question-fallacies) of the fact that _attractive(46+)_ and _attractive(80+)_ have _different meanings_, I might feel better about myself.

In political discourse, this kind of strategic equivocation is sometimes called [the motte and bailey doctrine](https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/): [TODO: it's still bad math even when you're being nice to someone]

You can't make the world better by changing your _description_ of the world; that's wireheading. Rather, you want your description to match reality, so that you can use it to help you _figure out_ how to make the world better.
