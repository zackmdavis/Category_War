Time for a Twitter-optimized capsule ðŸ§µ of my 2021 philosophy of language thesis about why poor choices of definitions are relevantly similar to lying! If you wouldn't lie, you also shouldn't say, "it's not lying; I'm just defining words in a way that I prefer." 1/23

Some people say: the borders of a category are like the borders of a countryâ€”they have consequences, but there's no sense in which some possible borders can be objectively worse than others. 2/23

But category "borders" or "boundaries" are just a visual metaphor corresponding to a kind of probabilistic model. Editing the "boundary" means editing the model's predictions. There is a sense in which some models can be objectively worse than others! 3/23

Imagine having to sort a bunch of blue egg-shaped things (which contain vanadium) & red cubes (that don't). Technically, you don't actually need separate "blue egg" & "red cube" categories. You could just build up a joint probability table over all objects and query that. 4/23

But that's unwieldy. Thinking about "blue egg" & "red cube" as separate categories and computing the properties of an object conditional on its category is much more efficient. 5/23

"Computing the properties of an object conditional on its category" can be visualized as category "boundaries" in a picture. 6/23

But the picture is an illustration of the math; you can't change the picture without changing the math. It's not like national borders at all. The U.S. purchasing Alaska (non-contiguous with the 48 states) wasn't about editing a probabilistic model. 7/23

In itself, this doesn't yet explain what's wrong with "squiggly", "gerrymandered" categories. You can still make predictions with squiggly categories. 8/23

But if approximately-correct answers are at all more useful than totally-wrong answers, squiggly categories are mathematically just worse (going by the mean squared error). If your "blue eggs" category contains some red cubes, you'll look for vanadium where there isn't any. 9/23

The best categories are subjective in the sense that they depend on what you're trying to predict, but that's not the same thing as the category boundary itself being subjective. Given what you want to predict, the model (and thus the "boundary") is determined by the data. 10/23

Some people say: okay, but what if I really do have a preference for using a particular squiggly boundary, intrinsically, not in a way that arises from desired predictions and the data distribution? That's just how my utility function is! What's irrational about that? 11/23

Let's interrogate this. What would it mean, to have such an exotic utility function? There is a trivial sense in which any pattern of behavior, however bizarre, could be rationalized in terms of preferring to take the actions that I do in the situation that I face. 12/23

But a theory that explains everything explains nothing. The explanatory value of the "utility function" formalism isn't that it can justify anything given a choice of "utility", but in the constraints it articulates on coherent behaviors (given, yes, a choice of "utility"). 13/23

If your gambling behavior violates the independence axiom with respect to money, that doesn't automatically make you irrational, but it does mean that you're acting as if you care about something else besides moneyâ€”that you'll sacrifice some money for that something else. 14/23

Similarly, if your communication signals aren't explainable in terms of conveying probabilistic predictions, that does imply that you care about something else than conveying probabilistic predictionsâ€”that you'll sacrifice clarity (of predictions) for that something else. 15/23

But what might that something else be, concretely? It's quite hard to see where a completely arbitrary, hardwired, "just because" preference for using a particular category boundary would come from! Why would that be a thing? Why?? 16/23

A much more plausible reason to sacrifice clarity of predictions is because you don't want other agents to make accurate predictions. (Because if those others had better models, they'd make decisions that harm your interests.) That's deception. 17/23

There's no functional difference between saying "I reserve the right to lie p% of the time about whether something belongs to a category" and adopting a new category system that misclassifies p% of things. The inputâ€“output relations are the same. 18/23

A related reason: it's tempting to "wirehead" by choosing a map that looks good, instead of the map that reflects the territory (which might be unpleasant to look at). That's self-deception. 19/23

If I want to believe I'm pretty & funny, it might be tempting to redefine "pretty" & "funny" such that they include me. But that would just be fooling myself; it doesn't actually work for making me pretty & funny (with respect to the usual meanings). 20/23

Sometimes things resemble another in some but not all aspects. This is mimickry. It's deceptive if the point is for another agent to treat the mimic as the original against the agent's interestsâ€”but it's not deceptive if the agent really doesn't care about the difference. 21/23

If agents sharing a language disagree about which aspects "count", they'll fight over the definitions of words: animal advocates would prefer if plant-based meat substitutes counted as "real" meat, to make it hard for carnivores to insist on the dead-animal kind. 22/23

Philosophy itself can't determine which definition is right (which depends on the empirical merits), but philosophy does clarify what's happening in this kind of conflictâ€”that departing from the empirical merits extracts a cost in the form of worse predictions. END/23
